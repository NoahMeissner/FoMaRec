{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e394fe5",
   "metadata": {},
   "source": [
    "# Ablaition Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e8b218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import json\n",
    "from foodrec.config.structure.dataset_enum import ModelEnum \n",
    "from foodrec.evaluation.is_ketogen import calc_keto_ratio\n",
    "from foodrec.config.structure.paths import CONVERSATION, DATASET_PATHS\n",
    "from foodrec.evaluation.is_ketogen import calc_keto_ratio\n",
    "from foodrec.evaluation.reward_evaluation import routing_accuracy\n",
    "from analysis_helper.load_dataset import get_dicts_set, get_search_engine\n",
    "from analysis_helper.get_lowes_highest import take_25_lowest_keto\n",
    "from analysis_helper.get_metrics import calc_metrics\n",
    "from analysis_helper.mean_rounds import calc_rounds\n",
    "from analysis_helper.query_analysis import calc_other_recommendation_parameters\n",
    "from analysis_helper.calc_routing_reward import get_reward_set, reward_average_calculation\n",
    "from analysis_helper.calc_path_length import calc_path_length\n",
    "from analysis_helper.time import calc_mean_time\n",
    "from analysis_helper.reflector_analysis import cals_reflector_accuracy\n",
    "from analysis_helper.ketogen_available import ketogen_available\n",
    "from analysis_helper.load_dataset import get_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36091659",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_set = pd.read_csv(DATASET_PATHS / \"zw_personas.csv\")\n",
    "model = ModelEnum.Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6817372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths(model_name):\n",
    "    return {\n",
    "        \"PATH_NO_BIASE\": CONVERSATION / model_name / \"no_biase\",\n",
    "        \"PATH_SYSTEM_BIASE\": CONVERSATION / model_name / \"system_biase\",\n",
    "        \"PATH_SEARCH_ENGINE\": CONVERSATION / \"search_engine\" / \"res_one.json\",\n",
    "        \"PATH_SEARCH_BIASE\": CONVERSATION / model_name / \"search_biase\" ,\n",
    "        \"PATH_BOTH\": CONVERSATION / model_name / \"both_biase\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e169634",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = get_paths(ModelEnum.Gemini.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85e43e1",
   "metadata": {},
   "source": [
    "## Reasons for Hypothesis Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b139aa09",
   "metadata": {},
   "source": [
    "### task success lowest 25 highest 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eb33545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25%: 0.2624075915346725\n",
      "75%: 0.4752273733006868\n",
      "Mean: 0.4078187139691796\n",
      "Max: 1.565975754772325\n",
      "28\n",
      "2.1010442773600673 25 9.357142857142858 25\n",
      "21.5 25 19.09621212121212 25\n",
      "10.342857142857138 100 55.0 100\n",
      "24.0 25 13.66678845451224 25\n",
      "25.0 25 9.521862948077782 25\n",
      "               Macro Precision  Macro Recall  Macro F1  Micro Precision  \\\n",
      "Bias                                                                      \n",
      "No Biase              0.084042      0.374286  0.137263         0.105590   \n",
      "System Biase          0.860000      0.763848  0.809078         0.982143   \n",
      "Search Engine         0.103429      0.550000  0.174115         0.104569   \n",
      "Search Biase          0.960000      0.546672  0.696641         1.000000   \n",
      "Both Biase            1.000000      0.380875  0.551642         1.000000   \n",
      "\n",
      "               Micro Recall  Micro F1  Mean Average Precision  Mean PR-AUC  \\\n",
      "Bias                                                                         \n",
      "No Biase           0.414634  0.168317                0.205998     0.230048   \n",
      "System Biase       0.753425  0.852713                0.272431     0.880000   \n",
      "Search Engine      1.000000  0.189338                0.182551     0.182551   \n",
      "Search Biase       0.494545  0.661800                0.993929     0.960000   \n",
      "Both Biase         0.307263  0.470085                0.985555     1.000000   \n",
      "\n",
      "               Mean Length of Search Results  Mean Response Length  \\\n",
      "Bias                                                                 \n",
      "No Biase                               12.96              7.000000   \n",
      "System Biase                           16.88              2.545455   \n",
      "Search Engine                           9.85              9.850000   \n",
      "Search Biase                           11.08              5.666667   \n",
      "Both Biase                             14.52              4.400000   \n",
      "\n",
      "               Median Hit Length  Bias Conformity@1  Bias Conformity@3  \\\n",
      "Bias                                                                     \n",
      "No Biase                0.500000           0.173913           0.115942   \n",
      "System Biase            0.119048           1.000000           0.977273   \n",
      "Search Engine           1.000000           0.100000           0.080000   \n",
      "Search Biase            0.500000           1.000000           1.000000   \n",
      "Both Biase              0.400000           1.000000           1.000000   \n",
      "\n",
      "               Bias Conformity@5  Accuracy  \n",
      "Bias                                        \n",
      "No Biase                0.103623  0.173913  \n",
      "System Biase            0.977273  1.000000  \n",
      "Search Engine           0.096000  0.100000  \n",
      "Search Biase            1.000000  1.000000  \n",
      "Both Biase              1.000000  1.000000  \n"
     ]
    }
   ],
   "source": [
    "q_set_highest = take_25_lowest_keto(query_set,paths, high=True)\n",
    "pr_auc_raw_high = calc_metrics(query_set=q_set_highest, paths=paths, model_name=ModelEnum.Gemini, ref_include=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcccc361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25%: 0.2624075915346725\n",
      "75%: 0.4752273733006868\n",
      "Mean: 0.4078187139691796\n",
      "Max: 1.565975754772325\n",
      "34\n",
      "1.4023809523809525 25 6.5 25\n",
      "13.666666666666668 25 14.833333333333334 25\n",
      "10.342857142857138 100 55.0 100\n",
      "21.617424242424242 25 12.972010929179714 25\n",
      "23.5 25 7.266743480014867 25\n",
      "               Macro Precision  Macro Recall  Macro F1  Micro Precision  \\\n",
      "Bias                                                                      \n",
      "No Biase              0.056095      0.260000  0.092281         0.081301   \n",
      "System Biase          0.546667      0.593333  0.569045         0.675000   \n",
      "Search Engine         0.103429      0.550000  0.174115         0.104569   \n",
      "Search Biase          0.864697      0.518880  0.648571         0.930070   \n",
      "Both Biase            0.940000      0.290670  0.444034         0.988095   \n",
      "\n",
      "               Micro Recall  Micro F1  Mean Average Precision  Mean PR-AUC  \\\n",
      "Bias                                                                         \n",
      "No Biase           0.500000  0.139860                0.119505     0.094921   \n",
      "System Biase       0.900000  0.771429                0.089824     0.620000   \n",
      "Search Engine      1.000000  0.189338                0.182551     0.182551   \n",
      "Search Biase       0.405488  0.564756                0.992027     0.868439   \n",
      "Both Biase         0.213918  0.351695                0.975527     0.940000   \n",
      "\n",
      "               Mean Length of Search Results  Mean Response Length  \\\n",
      "Bias                                                                 \n",
      "No Biase                               10.76              4.920000   \n",
      "System Biase                           19.40              2.000000   \n",
      "Search Engine                           9.85              9.850000   \n",
      "Search Biase                           13.36              5.958333   \n",
      "Both Biase                             15.76              3.500000   \n",
      "\n",
      "               Median Hit Length  Bias Conformity@1  Bias Conformity@3  \\\n",
      "Bias                                                                     \n",
      "No Biase                0.500000           0.000000           0.066667   \n",
      "System Biase            0.083333           0.750000           0.683333   \n",
      "Search Engine           1.000000           0.100000           0.080000   \n",
      "Search Biase            0.473684           0.875000           0.888889   \n",
      "Both Biase              0.200000           0.958333           0.979167   \n",
      "\n",
      "               Bias Conformity@5  Accuracy  \n",
      "Bias                                        \n",
      "No Biase                0.050000  0.000000  \n",
      "System Biase            0.683333  0.750000  \n",
      "Search Engine           0.096000  0.100000  \n",
      "Search Biase            0.897917  0.875000  \n",
      "Both Biase              0.979167  0.958333  \n"
     ]
    }
   ],
   "source": [
    "q_set_low = take_25_lowest_keto(query_set,paths, high=False)\n",
    "pr_auc_raw_low = calc_metrics(query_set=q_set_low, paths=paths, model_name=ModelEnum.Gemini, ref_include=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9391fa71",
   "metadata": {},
   "source": [
    "### No Reflector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1221c798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_no_improvment(df, paths, model_name: ModelEnum):\n",
    "    def calc_median_rounds(df, model:ModelEnum, Path):\n",
    "        ls = []\n",
    "        for index, row in df.iterrows():\n",
    "            persona_id = row[\"id\"]\n",
    "            query = row[\"query\"]\n",
    "            ls.append(cals_reflector_accuracy(persona_id=persona_id, query=query, model=model, Path=Path))\n",
    "        return ls\n",
    "    ls_no_biase = calc_median_rounds(df, model_name, paths['PATH_NO_BIASE'])\n",
    "    ls_system_biase = calc_median_rounds(df, model_name, paths['PATH_SYSTEM_BIASE'])\n",
    "    ls_search_biase = calc_median_rounds(df, model_name, paths['PATH_SEARCH_BIASE'])\n",
    "    ls_both_biase = calc_median_rounds(df, model_name, paths['PATH_BOTH'])\n",
    "\n",
    "    print(f\"Mean Rounds No Biase: Accuracy:{np.mean([True if keto > 0.8 else False for keto in ls_no_biase])}  Mean Rate {np.mean(ls_no_biase)}\")\n",
    "    print(f\"Mean Rounds System Biase:  Accuracy:{np.mean([True if keto > 0.8 else False for keto in ls_system_biase])}  Mean Rate {np.mean(ls_system_biase)}\")\n",
    "    print(f\"Mean Search Biase:  Accuracy:{np.mean([True if keto > 0.8 else False for keto in ls_search_biase])}  Mean Rate {np.mean(ls_search_biase)}\")\n",
    "    print(f\"Mean Both Biase:  Accuracy:{np.mean([True if keto > 0.8 else False for keto in ls_both_biase])}  Mean Rate {np.mean(ls_both_biase)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "853839f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rounds No Biase: Accuracy:0.11  Mean Rate 0.41720981764709003\n",
      "Mean Rounds System Biase:  Accuracy:0.63  Mean Rate 0.9009933569286928\n",
      "Mean Search Biase:  Accuracy:0.94  Mean Rate 1.2604315432172488\n",
      "Mean Both Biase:  Accuracy:0.98  Mean Rate 1.7588926057664371\n"
     ]
    }
   ],
   "source": [
    "calc_no_improvment(query_set, paths, ModelEnum.Gemini)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543e8754",
   "metadata": {},
   "source": [
    "### No Ketogen Recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d407a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def was_ketogen_available(df, paths, model_name: ModelEnum):\n",
    "    def calc_median_rounds(df, model:ModelEnum, Path):\n",
    "        ls = []\n",
    "        for index, row in df.iterrows():\n",
    "            persona_id = row[\"id\"]\n",
    "            query = row[\"query\"]\n",
    "            ls.append(ketogen_available(persona_id=persona_id, query=query, model=model, Path=Path))\n",
    "        filtered = [x for x in ls if x is not None]\n",
    "        return filtered\n",
    "    ls_no_biase = calc_median_rounds(df, model_name, paths['PATH_NO_BIASE'])\n",
    "    ls_system_biase = calc_median_rounds(df, model_name, paths['PATH_SYSTEM_BIASE'])\n",
    "    ls_search_biase = calc_median_rounds(df, model_name, paths['PATH_SEARCH_BIASE'])\n",
    "    ls_both_biase = calc_median_rounds(df, model_name, paths['PATH_BOTH'])\n",
    "\n",
    "\n",
    "    print(f\"Mean Rounds No Biase: Accuracy: {np.mean(ls_no_biase)}\")\n",
    "    print(f\"Mean Rounds search Biase: Accuracy: {np.mean(ls_search_biase)}\")\n",
    "    print(f\"Mean Rounds both Biase: Accuracy: {np.mean(ls_both_biase)}\")\n",
    "\n",
    "    print(f\"Mean Rounds System Biase: {np.mean(ls_system_biase)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45b91df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "unsupported operand type(s) for +: 'NoneType' and 'NoneType'\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "unsupported operand type(s) for +: 'NoneType' and 'NoneType'\n",
      "list index out of range\n",
      "unsupported operand type(s) for +: 'NoneType' and 'NoneType'\n",
      "list index out of range\n",
      "list index out of range\n",
      "Mean Rounds No Biase: Accuracy: 0.6179775280898876\n",
      "Mean Rounds search Biase: Accuracy: 1.0\n",
      "Mean Rounds both Biase: Accuracy: 1.0\n",
      "Mean Rounds System Biase: 0.23076923076923078\n"
     ]
    }
   ],
   "source": [
    "was_ketogen_available(query_set, paths, ModelEnum.Gemini)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089dda64",
   "metadata": {},
   "source": [
    "## Reference Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b87db42",
   "metadata": {},
   "source": [
    "### Recommendations Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e0612cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendation_accuracy(df, model_name:ModelEnum):\n",
    "    paths = get_paths(str(model_name.name))    \n",
    "    print(10*\"-\"+\"PATH_NO_BIASE\"+10*\"-\")\n",
    "    calc_other_recommendation_parameters(df, paths[\"PATH_NO_BIASE\"],model_name )\n",
    "    print(10*\"-\"+\"PATH_SYSTEM_BIASE\"+10*\"-\")\n",
    "    calc_other_recommendation_parameters(df, paths[\"PATH_SYSTEM_BIASE\"],model_name )\n",
    "    print(10*\"-\"+\"PATH_BOTH\"+10*\"-\")\n",
    "    calc_other_recommendation_parameters(df, paths[\"PATH_BOTH\"], model_name )\n",
    "    print(10*\"-\"+\"PATH_SEARCH_BIASE\"+10*\"-\")\n",
    "    calc_other_recommendation_parameters(df, paths[\"PATH_SEARCH_BIASE\"], model_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a426608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------PATH_NO_BIASE----------\n",
      "EmbeddingLoader initialised\n",
      "Like 0.5306122448979592\n",
      " Avoid 0.9912280701754386\n",
      "Cuisine 0.90625\n",
      "Overall0.8093634383577992\n",
      "----------PATH_SYSTEM_BIASE----------\n",
      "EmbeddingLoader initialised\n",
      "Like 0.4857142857142857\n",
      " Avoid 1.0\n",
      "Cuisine 0.9\n",
      "Overall0.7952380952380952\n",
      "----------PATH_BOTH----------\n",
      "EmbeddingLoader initialised\n",
      "Like 0.45652173913043476\n",
      " Avoid 0.990990990990991\n",
      "Cuisine 0.873015873015873\n",
      "Overall0.7735095343790995\n",
      "----------PATH_SEARCH_BIASE----------\n",
      "EmbeddingLoader initialised\n",
      "Like 0.45454545454545453\n",
      " Avoid 0.9906542056074766\n",
      "Cuisine 0.8524590163934426\n",
      "Overall0.7658862255154579\n"
     ]
    }
   ],
   "source": [
    "recommendation_accuracy(query_set, ModelEnum.OpenAI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a1e2f2",
   "metadata": {},
   "source": [
    "### Descriptive Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e4717b",
   "metadata": {},
   "source": [
    "### Recommendation Accuracy Ketogen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba690020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.904761904761903 100 22.978571428571428 100\n",
      "75.0 100 61.3734126984127 100\n",
      "10.342857142857138 100 55.0 100\n",
      "91.0 100 41.22900241126103 100\n",
      "94.0 100 34.003591546092125 100\n",
      "               Macro Precision  Macro Recall  Macro F1  Micro Precision  \\\n",
      "Bias                                                                      \n",
      "No Biase              0.109048      0.229786  0.147905         0.103371   \n",
      "System Biase          0.750000      0.613734  0.675059         0.981366   \n",
      "Search Engine         0.103429      0.550000  0.174115         0.104569   \n",
      "Search Biase          0.910000      0.412290  0.567476         0.994667   \n",
      "Both Biase            0.940000      0.340036  0.499414         0.994652   \n",
      "\n",
      "               Micro Recall  Micro F1  Mean Average Precision  Mean PR-AUC  \\\n",
      "Bias                                                                         \n",
      "No Biase           0.340741  0.158621                0.211093     0.178813   \n",
      "System Biase       0.578755  0.728111                0.297362     0.750000   \n",
      "Search Engine      1.000000  0.189338                0.182551     0.182551   \n",
      "Search Biase       0.269314  0.423864                1.000000     0.910000   \n",
      "Both Biase         0.238462  0.384695                1.000000     0.940000   \n",
      "\n",
      "               Mean Length of Search Results  Mean Response Length  \\\n",
      "Bias                                                                 \n",
      "No Biase                               12.80              4.540816   \n",
      "System Biase                           21.51              2.064103   \n",
      "Search Engine                           9.85              9.850000   \n",
      "Search Biase                           13.85              4.032258   \n",
      "Both Biase                             15.60              3.936842   \n",
      "\n",
      "               Median Hit Length  Bias Conformity@1  Bias Conformity@3  \\\n",
      "Bias                                                                     \n",
      "No Biase                0.500000           0.091837           0.119048   \n",
      "System Biase            0.080128           0.961538           0.961538   \n",
      "Search Engine           1.000000           0.100000           0.080000   \n",
      "Search Biase            0.362782           0.978495           0.978495   \n",
      "Both Biase              0.300000           0.989474           0.989474   \n",
      "\n",
      "               Bias Conformity@5  Accuracy  \n",
      "Bias                                        \n",
      "No Biase                0.109014  0.091837  \n",
      "System Biase            0.961538  0.961538  \n",
      "Search Engine           0.096000  0.100000  \n",
      "Search Biase            0.978495  0.978495  \n",
      "Both Biase              0.989474  0.989474  \n"
     ]
    }
   ],
   "source": [
    "paths = get_paths(ModelEnum.OpenAI.name)\n",
    "pr_auc_raw = calc_metrics(query_set=query_set, paths=paths, model_name=ModelEnum.OpenAI, ref_include=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a69ad4",
   "metadata": {},
   "source": [
    "### Mean Rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0692cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean_rounds(df, paths, model_name: ModelEnum):\n",
    "    def calc_median_rounds(df, model:ModelEnum, Path):\n",
    "        ls = []\n",
    "        for index, row in df.iterrows():\n",
    "            persona_id = row[\"id\"]\n",
    "            query = row[\"query\"]\n",
    "            ls.append(calc_rounds(persona_id=persona_id, query=query, model=model, Path=Path))\n",
    "        return np.mean(ls)\n",
    "    print(\"Mean Rounds No Biase:\", calc_median_rounds(df, model_name, paths['PATH_NO_BIASE']))\n",
    "    print(\"Mean Rounds System Biase:\", calc_median_rounds(df, model_name, paths['PATH_SYSTEM_BIASE']))\n",
    "    print(\"Mean Search Biase:\", calc_median_rounds(df, model_name, paths['PATH_SEARCH_BIASE']))\n",
    "    print(\"Mean Both Biase:\", calc_median_rounds(df, model_name, paths['PATH_BOTH']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "785506d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rounds No Biase: 1.57\n",
      "Mean Rounds System Biase: 3.02\n",
      "Mean Search Biase: 2.03\n",
      "Mean Both Biase: 2.15\n"
     ]
    }
   ],
   "source": [
    "calc_mean_rounds(query_set, paths=paths, model_name=ModelEnum.OpenAI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e914428c",
   "metadata": {},
   "source": [
    "### Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69a18b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_reward(query_set, paths, model_name: ModelEnum):\n",
    "    reward_system_biase = get_reward_set(query_set, model_name, paths['PATH_SYSTEM_BIASE'])\n",
    "    reward_system_no = get_reward_set(query_set, model_name, paths['PATH_NO_BIASE'])\n",
    "    reward_search_biase = get_reward_set(query_set, model_name, paths['PATH_SEARCH_BIASE'])\n",
    "    reward_both = get_reward_set(query_set, model_name, paths['PATH_BOTH'])\n",
    "    print(f\"Durchschnittlicher Score für System Biase: {reward_average_calculation(reward_system_biase)}\")\n",
    "\n",
    "    print(f\"Durchschnittlicher Score fuer No Biase: {reward_average_calculation(reward_system_no)}\")\n",
    "    \n",
    "    print(f\"Durchschnittlicher Score für Search Biase: {reward_average_calculation(reward_search_biase)}\")\n",
    "    \n",
    "    print(f\"Durchschnittlicher Score für Both Biase: {reward_average_calculation(reward_both)}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9e849a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Durchschnittlicher Score für System Biase: Score: 0.6136 bei gamma=1, normalize=True\n",
      "Durchschnittlicher Score fuer No Biase: Score: 0.4841 bei gamma=1, normalize=True\n",
      "Durchschnittlicher Score für Search Biase: Score: 0.5259 bei gamma=1, normalize=True\n",
      "Durchschnittlicher Score für Both Biase: Score: 0.5391 bei gamma=1, normalize=True\n"
     ]
    }
   ],
   "source": [
    "calc_reward(query_set=query_set, paths=paths, model_name=ModelEnum.OpenAI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296322a3",
   "metadata": {},
   "source": [
    "### Path Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "251c514c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_average_path_length(df, paths, model_name: ModelEnum):\n",
    "    def calc_median_path_length(df, model:ModelEnum, Path):\n",
    "        ls = []\n",
    "        for index, row in df.iterrows():\n",
    "            persona_id = row[\"id\"]\n",
    "            query = row[\"query\"]\n",
    "            ls.append(calc_path_length(persona_id=persona_id, query=query, model=model, path=Path))\n",
    "        return np.mean(ls)\n",
    "\n",
    "    print(\"Mean Path Length No Biase:\", calc_median_path_length(df, model_name, paths['PATH_NO_BIASE']))\n",
    "    print(\"Mean Path Length Biase:\", calc_median_path_length(df, model_name, paths['PATH_SYSTEM_BIASE']))\n",
    "    print(\"Mean Path Search Biase:\", calc_median_path_length(df, model_name, paths['PATH_SEARCH_BIASE']))\n",
    "    print(\"Mean Path Both Biase:\", calc_median_path_length(df, model_name, paths['PATH_BOTH']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64aa2252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Path Length No Biase: 6.75\n",
      "Mean Path Length Biase: 11.19\n",
      "Mean Path Search Biase: 8.14\n",
      "Mean Path Both Biase: 8.56\n"
     ]
    }
   ],
   "source": [
    "calc_average_path_length(df=query_set,paths=paths, model_name=ModelEnum.OpenAI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb62c70",
   "metadata": {},
   "source": [
    "### Routing Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8a6b5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_routing_accuracy(query_set, paths, model_name: ModelEnum):\n",
    "    def routing_accuracy_calculation(reward_system):\n",
    "        scores = []\n",
    "        for i, episode in enumerate(reward_system, start=1):\n",
    "            score = routing_accuracy(episode)\n",
    "            scores.append(score)\n",
    "\n",
    "        # Optional: Gesamtauswertung\n",
    "        avg_score = np.mean(scores)\n",
    "        return f\"Score: {avg_score:.4f}\"\n",
    "    reward_system_biase = get_reward_set(query_set, model_name, paths['PATH_SYSTEM_BIASE'])\n",
    "    reward_system_no = get_reward_set(query_set, model_name, paths['PATH_NO_BIASE'])\n",
    "    reward_search_biase = get_reward_set(query_set, model_name, paths['PATH_SEARCH_BIASE'])\n",
    "    reward_both = get_reward_set(query_set, model_name, paths['PATH_BOTH'])\n",
    "\n",
    "    print(f\"Durchschnittlicher Score für System Biase: {routing_accuracy_calculation(reward_system_biase)}\")\n",
    "    print(f\"Durchschnittlicher Score fuer No Biase: {routing_accuracy_calculation(reward_system_no)}\")\n",
    "    print(f\"Durchschnittlicher Score für Search Biase: {routing_accuracy_calculation(reward_search_biase)}\")\n",
    "    print(f\"Durchschnittlicher Score fuer Both Biase: {routing_accuracy_calculation(reward_both)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ea70925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Durchschnittlicher Score für System Biase: Score: 0.8067\n",
      "Durchschnittlicher Score fuer No Biase: Score: 0.7418\n",
      "Durchschnittlicher Score für Search Biase: Score: 0.7633\n",
      "Durchschnittlicher Score fuer Both Biase: Score: 0.7690\n"
     ]
    }
   ],
   "source": [
    "calc_routing_accuracy(query_set=query_set,paths=paths, model_name=ModelEnum.OpenAI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b126cf",
   "metadata": {},
   "source": [
    "### Mean Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e91dcbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Time No Biase: 308.31\n",
      "Mean Time System Biase: 493.52\n",
      "Mean Time Search Biase: 357.86\n",
      "Mean Both Biase: 406.13\n"
     ]
    }
   ],
   "source": [
    "calc_mean_time(query_set, paths=paths, model_name=ModelEnum.OpenAI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1c5820",
   "metadata": {},
   "source": [
    "### Last Reflector Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "775d0a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_reflector_answer(persona_id: int, query: str, model: ModelEnum, Path = None):\n",
    "    filepath = get_file_path(Path=Path, query=query, persona_id=persona_id, model=model)\n",
    "    reflector = {}\n",
    "    if filepath == None:\n",
    "        return 0\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                \n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    obj = json.loads(line)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue  # kaputte Zeilen überspringen\n",
    "                if obj.get(\"role\") == \"REFLECTOR\":\n",
    "                    reflector= obj\n",
    "            meta = reflector[\"meta\"]\n",
    "            decision = meta['decision']\n",
    "            if decision.lower() == \"accept\":\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e55f823",
   "metadata": {},
   "source": [
    "### Task Success Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a8398c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_task_success_rate(query_set, paths, model:ModelEnum):\n",
    "    def calc_individual_rate(query_set, Path, model:ModelEnum):\n",
    "        ls = []\n",
    "        for index, row in query_set.iterrows():\n",
    "            persona_id = row[\"id\"]\n",
    "            query = row[\"query\"]\n",
    "            ls.append(get_last_reflector_answer(persona_id=persona_id, query=query, model=model, Path=Path))\n",
    "        return np.mean(ls), ls     \n",
    "    no_biase_mean, no_biase_raw =   calc_individual_rate(query_set= query_set, model=model, Path=paths['PATH_NO_BIASE'])\n",
    "    system_biase, system_biase_raw = calc_individual_rate(query_set, model=model, Path=paths['PATH_SYSTEM_BIASE'])\n",
    "    search_biase, search_biase_raw = calc_individual_rate(query_set=query_set, model=model, Path=paths['PATH_SEARCH_BIASE'])\n",
    "    both_biase, search_both_biase = calc_individual_rate(query_set=query_set, model=model, Path=paths['PATH_BOTH'])\n",
    "    print(\"Task Success Rate No Biase:\", no_biase_mean)\n",
    "    print(\"Task Success Rate Biase:\", system_biase)\n",
    "    print(\"Task Success Rate Search Biase:\", search_biase)\n",
    "    print(\"Task Success Rate Both Biase:\", both_biase)\n",
    "    return no_biase_raw, system_biase_raw, search_biase_raw, search_both_biase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "edf61d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Success Rate No Biase: 0.93\n",
      "Task Success Rate Biase: 0.6\n",
      "Task Success Rate Search Biase: 0.84\n",
      "Task Success Rate Both Biase: 0.83\n"
     ]
    }
   ],
   "source": [
    "no_biase_raw, system_biase_raw, search_biase_raw, search_both_biase = calc_task_success_rate(query_set=query_set, paths=paths,model=ModelEnum.OpenAI)\n",
    "eval_h2 = {\n",
    "    \"SystemBiase\": system_biase_raw,\n",
    "    \"NoBiase\":no_biase_raw,\n",
    "    \"SearchBiase\":search_biase_raw,\n",
    "    \"BothBiase\":search_both_biase\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7af44a",
   "metadata": {},
   "source": [
    "### Calc Ketogen Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ab1f754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_keto(dict_biase: dict, df, keto_ratio_index: float = 0.8, mode: str = \"top1\") -> dict:\n",
    "    \"\"\"\n",
    "    dict_biase: { query -> [ {recipe_dict}, ... ] } ODER { query -> {recipe_dict} }\n",
    "    mode: 'top1' (nur erstes Rezept) oder 'any' (mind. ein Rezept ist ketogen)\n",
    "    return: { query -> 0/1 }\n",
    "    \"\"\"\n",
    "    def _is_keto(rec: dict) -> bool:\n",
    "        obj = calc_keto_ratio(\n",
    "            protein_g=rec.get(\"proteins\", 0),\n",
    "            fat_g=rec.get(\"fat\", 0),\n",
    "            carbs_g=rec.get(\"carbohydrates\", 0),\n",
    "        )\n",
    "        if obj == np.inf:\n",
    "            return 0\n",
    "        return obj\n",
    "\n",
    "    res = {}\n",
    "    for _, row in df.iterrows():\n",
    "        q = row[\"query\"]\n",
    "\n",
    "        # existiert der Query überhaupt?\n",
    "        if q not in dict_biase:\n",
    "            res[q] = 0\n",
    "            continue\n",
    "\n",
    "        items = dict_biase[q]\n",
    "\n",
    "        # Normalisieren auf Liste von Rezepten\n",
    "        if items is None:\n",
    "            res[q] = 0\n",
    "            continue\n",
    "        if isinstance(items, dict):\n",
    "            items = [items]\n",
    "        if not isinstance(items, (list, tuple)) or len(items) == 0:\n",
    "            res[q] = 0\n",
    "            continue\n",
    "\n",
    "        # Modus: top1 oder any\n",
    "        if mode == \"top1\":\n",
    "            res[q] = float(_is_keto(items[0]))\n",
    "        elif mode == \"any\":\n",
    "            res[q] = float(any(_is_keto(r) for r in items))\n",
    "        else:\n",
    "            raise ValueError(\"mode must be 'top1' or 'any'\")\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2780565",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_search_engine, dict_search_engine_search = get_search_engine(paths['PATH_SEARCH_ENGINE'])\n",
    "dict_system_biase,  dict_system_biase_search, ref_system_biase  = get_dicts_set(df=query_set, model=ModelEnum.OpenAI, Path=paths['PATH_SYSTEM_BIASE'])\n",
    "dict_no_biase,      dict_no_biase_search, ref_no_biase      = get_dicts_set(df=query_set, model=ModelEnum.OpenAI, Path=paths['PATH_NO_BIASE'])\n",
    "dict_search_biase,  dict_search_biase_search, ref_search_biase  = get_dicts_set(query_set, ModelEnum.OpenAI, paths['PATH_SEARCH_BIASE'])\n",
    "dict_both,          dict_both_search, ref_both_biase          = get_dicts_set(query_set, ModelEnum.OpenAI, paths['PATH_BOTH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1124a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = {\n",
    "    \"SystemBiase\": calc_keto(df=query_set, dict_biase=dict_system_biase),\n",
    "    \"NoBiase\":calc_keto(df=query_set, dict_biase=dict_no_biase),\n",
    "    \"SearchEngine\": calc_keto(df=query_set, dict_biase=dict_search_engine),\n",
    "    \"SearchBiase\":calc_keto(df=query_set, dict_biase=dict_search_biase),\n",
    "    \"BothBiase\":calc_keto(df=query_set, dict_biase=dict_both)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce0d83d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_wide = pd.DataFrame(eval)              \n",
    "df_wide = df_wide.reset_index()            \n",
    "df_wide = df_wide.rename(columns={\"index\": \"query\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ad69dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SystemBiase Mean: 1.010412667903395\n",
      "NoBiase Mean: 0.2356610811198101\n",
      "SearchEngine Mean: 0.2503780723728942\n",
      "SearchBiase Mean: 0.9819557717116554\n",
      "BothBiase Mean: 1.1648577159671825\n"
     ]
    }
   ],
   "source": [
    "for col in df_wide.columns:\n",
    "    if col == \"query\": \n",
    "        continue\n",
    "    x = df_wide[col].dropna().to_numpy()\n",
    "    print(f\"{col} Mean: {np.median(x)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2e19be",
   "metadata": {},
   "source": [
    "### Costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d62baa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_open_ai_costs(df, paths, model_name: ModelEnum):\n",
    "    def calc_costs(df, model:ModelEnum, Path):\n",
    "        ls_costs = []\n",
    "        ls_input_token = []\n",
    "        ls_output_token = []\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            persona_id = row[\"id\"]\n",
    "            query = row[\"query\"]\n",
    "            try:\n",
    "                input_token, output_token, costs = calc_openai_costs(persona_id=persona_id, query=query, model=model, Path=Path)\n",
    "                ls_costs.append(costs)\n",
    "                ls_input_token.append(input_token)\n",
    "                ls_output_token.append(output_token)\n",
    "            except: \n",
    "                print(query)\n",
    "                continue\n",
    "        return ls_input_token, ls_output_token, ls_costs\n",
    "    \n",
    "    ls_input_token_no, ls_output_token_no, ls_costs_no = calc_costs(df, model_name, paths['PATH_NO_BIASE'])\n",
    "    ls_input_token_system, ls_output_token_system, ls_costs_system = calc_costs(df, model_name, paths['PATH_SYSTEM_BIASE'])\n",
    "    ls_input_token_search, ls_output_token_search, ls_costs_search = calc_costs(df, model_name, paths['PATH_SEARCH_BIASE'])\n",
    "    ls_input_token_both, ls_output_token_both, ls_costs_both = calc_costs(df, model_name, paths['PATH_BOTH'])\n",
    "\n",
    "    total_costs = sum(ls_costs_no) + sum(ls_costs_system) + sum(ls_costs_search) + sum(ls_costs_both)\n",
    "    print(len(ls_costs_no))\n",
    "    print(f'Total Costs: ${total_costs}')\n",
    "    print(f\"Costs No Biase: ${np.mean(ls_costs_no)} \")\n",
    "    print(f\"Costs System Biase: ${np.mean(ls_costs_system)}\")\n",
    "    print(f\"Costs Search Biase: ${np.mean(ls_costs_search)}\")\n",
    "    print(f\"Costs Both Biase: ${np.mean(ls_costs_both)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "23d06e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Total Costs: $20.091077750000004\n",
      "Costs No Biase: $0.03790495000000001 \n",
      "Costs System Biase: $0.06465689499999999\n",
      "Costs Search Biase: $0.045768772500000006\n",
      "Costs Both Biase: $0.052580160000000015\n"
     ]
    }
   ],
   "source": [
    "calc_open_ai_costs(df=query_set,paths=paths, model_name=ModelEnum.OpenAI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f23fb6",
   "metadata": {},
   "source": [
    "## Comparison with Gemini Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33da32d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------PATH_NO_BIASE----------\n",
      "EmbeddingLoader initialised\n",
      "Like 0.5510204081632653\n",
      " Avoid 0.9823008849557522\n",
      "Cuisine 0.8461538461538461\n",
      "Overall0.7931583797576213\n",
      "----------PATH_SYSTEM_BIASE----------\n",
      "EmbeddingLoader initialised\n",
      "Like 0.5121951219512195\n",
      " Avoid 0.9900990099009901\n",
      "Cuisine 0.8392857142857143\n",
      "Overall0.780526615379308\n",
      "----------PATH_BOTH----------\n",
      "EmbeddingLoader initialised\n",
      "Like 0.5128205128205128\n",
      " Avoid 0.9897959183673469\n",
      "Cuisine 0.9655172413793104\n",
      "Overall0.8227112241890567\n",
      "----------PATH_SEARCH_BIASE----------\n",
      "EmbeddingLoader initialised\n",
      "Like 0.5641025641025641\n",
      " Avoid 0.98989898989899\n",
      "Cuisine 0.8771929824561403\n",
      "Overall0.8103981788192315\n"
     ]
    }
   ],
   "source": [
    "recommendation_accuracy(query_set, ModelEnum.GEMINIPRO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe9af23",
   "metadata": {},
   "source": [
    "### Ketogen Accuracy Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "77799116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.866666666666667 100 22.87777777777778 100\n",
      "74.75 100 53.38748196248197 100\n",
      "10.342857142857138 100 55.0 100\n",
      "85.0 100 31.817554242023142 100\n",
      "85.0 100 21.447853239792103 100\n",
      "               Macro Precision  Macro Recall  Macro F1  Micro Precision  \\\n",
      "Bias                                                                      \n",
      "No Biase              0.108667      0.228778  0.147346         0.124688   \n",
      "System Biase          0.747500      0.533875  0.622880         0.853881   \n",
      "Search Engine         0.103429      0.550000  0.174115         0.104569   \n",
      "Search Biase          0.850000      0.318176  0.463028         1.000000   \n",
      "Both Biase            0.850000      0.214479  0.342528         1.000000   \n",
      "\n",
      "               Micro Recall  Micro F1  Mean Average Precision  Mean PR-AUC  \\\n",
      "Bias                                                                         \n",
      "No Biase           0.263158  0.169205                0.196583     0.177417   \n",
      "System Biase       0.504043  0.633898                0.222287     0.758056   \n",
      "Search Engine      1.000000  0.189338                0.182551     0.182551   \n",
      "Search Biase       0.196550  0.328527                1.000000     0.850000   \n",
      "Both Biase         0.138635  0.243511                1.000000     0.850000   \n",
      "\n",
      "               Mean Length of Search Results  Mean Response Length  \\\n",
      "Bias                                                                 \n",
      "No Biase                               16.62              4.134021   \n",
      "System Biase                           27.39              2.517241   \n",
      "Search Engine                           9.85              9.850000   \n",
      "Search Biase                           16.23              3.752941   \n",
      "Both Biase                             23.01              3.752941   \n",
      "\n",
      "               Median Hit Length  Bias Conformity@1  Bias Conformity@3  \\\n",
      "Bias                                                                     \n",
      "No Biase                0.306250           0.123711           0.113402   \n",
      "System Biase            0.084220           0.873563           0.865900   \n",
      "Search Engine           1.000000           0.100000           0.080000   \n",
      "Search Biase            0.244048           1.000000           1.000000   \n",
      "Both Biase              0.149832           1.000000           1.000000   \n",
      "\n",
      "               Bias Conformity@5  Accuracy  \n",
      "Bias                                        \n",
      "No Biase                0.112027  0.123711  \n",
      "System Biase            0.859195  0.873563  \n",
      "Search Engine           0.096000  0.100000  \n",
      "Search Biase            1.000000  1.000000  \n",
      "Both Biase              1.000000  1.000000  \n"
     ]
    }
   ],
   "source": [
    "paths = get_paths(ModelEnum.GEMINIPRO.name)\n",
    "pr_auc_raw = calc_metrics(query_set=query_set, paths=paths, model_name=ModelEnum.GEMINIPRO, ref_include=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f46bac4",
   "metadata": {},
   "source": [
    "### Mean Rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "018edb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rounds No Biase: 2.18\n",
      "Mean Rounds System Biase: 4.25\n",
      "Mean Search Biase: 2.62\n",
      "Mean Both Biase: 3.89\n"
     ]
    }
   ],
   "source": [
    "calc_mean_rounds(query_set, paths=paths, model_name=ModelEnum.GEMINIPRO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37312ed",
   "metadata": {},
   "source": [
    "### Reward Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f7c3e8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Durchschnittlicher Score für System Biase: Score: 0.7317 bei gamma=1, normalize=True\n",
      "Durchschnittlicher Score fuer No Biase: Score: 0.5504 bei gamma=1, normalize=True\n",
      "Durchschnittlicher Score für Search Biase: Score: 0.5975 bei gamma=1, normalize=True\n",
      "Durchschnittlicher Score für Both Biase: Score: 0.7049 bei gamma=1, normalize=True\n"
     ]
    }
   ],
   "source": [
    "calc_reward(query_set=query_set, paths=paths, model_name=ModelEnum.GEMINIPRO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46e61ad",
   "metadata": {},
   "source": [
    "### Average Path Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "06133801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Path Length No Biase: 8.56\n",
      "Mean Path Length Biase: 14.75\n",
      "Mean Path Search Biase: 9.87\n",
      "Mean Path Both Biase: 13.69\n"
     ]
    }
   ],
   "source": [
    "calc_average_path_length(df=query_set,paths=paths, model_name=ModelEnum.GEMINIPRO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7a4ff8",
   "metadata": {},
   "source": [
    "### Routing Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bda44a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Durchschnittlicher Score für System Biase: Score: 0.8662\n",
      "Durchschnittlicher Score fuer No Biase: Score: 0.7746\n",
      "Durchschnittlicher Score für Search Biase: Score: 0.7988\n",
      "Durchschnittlicher Score fuer Both Biase: Score: 0.8524\n"
     ]
    }
   ],
   "source": [
    "calc_routing_accuracy(query_set=query_set,paths=paths, model_name=ModelEnum.GEMINIPRO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfb6c9a",
   "metadata": {},
   "source": [
    "### Mean Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5107cff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Time No Biase: 333.78\n",
      "Mean Time System Biase: 905.95\n",
      "Mean Time Search Biase: 395.26\n",
      "Mean Both Biase: 721.99\n"
     ]
    }
   ],
   "source": [
    "calc_mean_time(query_set, paths=paths, model_name=ModelEnum.GEMINIPRO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b1825a",
   "metadata": {},
   "source": [
    "### Task Success Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f9d1e2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Success Rate No Biase: 0.85\n",
      "Task Success Rate Biase: 0.33\n",
      "Task Success Rate Search Biase: 0.78\n",
      "Task Success Rate Both Biase: 0.43\n"
     ]
    }
   ],
   "source": [
    "no_biase_raw, system_biase_raw, search_biase_raw, search_both_biase = calc_task_success_rate(query_set=query_set, paths=paths,model=ModelEnum.GEMINIPRO)\n",
    "eval_h2 = {\n",
    "    \"SystemBiase\": system_biase_raw,\n",
    "    \"NoBiase\":no_biase_raw,\n",
    "    \"SearchBiase\":search_biase_raw,\n",
    "    \"BothBiase\":search_both_biase\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ce96950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_search_engine, dict_search_engine_search = get_search_engine(paths['PATH_SEARCH_ENGINE'])\n",
    "dict_system_biase,  dict_system_biase_search, ref_system_biase  = get_dicts_set(df=query_set, model=ModelEnum.GEMINIPRO, Path=paths['PATH_SYSTEM_BIASE'])\n",
    "dict_no_biase,      dict_no_biase_search, ref_no_biase      = get_dicts_set(df=query_set, model=ModelEnum.GEMINIPRO, Path=paths['PATH_NO_BIASE'])\n",
    "dict_search_biase,  dict_search_biase_search, ref_search_biase  = get_dicts_set(query_set, ModelEnum.GEMINIPRO, paths['PATH_SEARCH_BIASE'])\n",
    "dict_both,          dict_both_search, ref_both_biase          = get_dicts_set(query_set, ModelEnum.GEMINIPRO, paths['PATH_BOTH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "95390913",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = {\n",
    "    \"SystemBiase\": calc_keto(df=query_set, dict_biase=dict_system_biase),\n",
    "    \"NoBiase\":calc_keto(df=query_set, dict_biase=dict_no_biase),\n",
    "    \"SearchEngine\": calc_keto(df=query_set, dict_biase=dict_search_engine),\n",
    "    \"SearchBiase\":calc_keto(df=query_set, dict_biase=dict_search_biase),\n",
    "    \"BothBiase\":calc_keto(df=query_set, dict_biase=dict_both)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d7ed826e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide = pd.DataFrame(eval)              \n",
    "df_wide = df_wide.reset_index()            \n",
    "df_wide = df_wide.rename(columns={\"index\": \"query\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "07c9110b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SystemBiase Mean: 1.0010899274947511\n",
      "NoBiase Mean: 0.27783885940861475\n",
      "SearchEngine Mean: 0.2503780723728942\n",
      "SearchBiase Mean: 0.972796922504326\n",
      "BothBiase Mean: 1.0539998428126967\n"
     ]
    }
   ],
   "source": [
    "for col in df_wide.columns:\n",
    "    if col == \"query\": \n",
    "        continue\n",
    "    x = df_wide[col].dropna().to_numpy()\n",
    "    print(f\"{col} Mean: {np.median(x)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv (3.11.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
