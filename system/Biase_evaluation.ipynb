{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62871b7b",
   "metadata": {},
   "source": [
    "# Evaluation of the Mulit Agent System "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148e34ab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29f4f811",
   "metadata": {},
   "source": [
    "This Notebook is responsible for the evaluation of the Multi Agent System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b46dbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from foodrec.config.structure.dataset_enum import ModelEnum \n",
    "from foodrec.evaluation.create_dataset import create_dataset\n",
    "from foodrec.evaluation.is_ketogen import is_ketogenic, calc_keto_ratio\n",
    "from foodrec.config.structure.paths import CONVERSATION, DATASET_PATHS\n",
    "import json\n",
    "from foodrec.evaluation.metrics.metrics import macro_over_queries,filter_search, micro_over_queries, accuracy, f1_score, mean_average_precision_over_queries, mean_pr_auc_over_queries, bias_conformity_rate_at_k\n",
    "from foodrec.data.all_recipe import AllRecipeLoader\n",
    "from typing import Dict, List, Any, Tuple\n",
    "from collections import Counter\n",
    "from foodrec.agents.agent_names import AgentEnum\n",
    "from foodrec.tools.ingredient_normalizer import IngredientNormalisation\n",
    "from analysis_helper.load_dataset import check_availability\n",
    "from foodrec.config.structure.dataset_enum import DatasetEnum\n",
    "from foodrec.evaluation.reward_evaluation import final_episode_reward, routing_accuracy\n",
    "from datetime import datetime\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6fc0776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rounds(persona_id: int, query: str, model: ModelEnum, Path = None):\n",
    "    query_stempt = query.replace(\" \", \"_\").lower()\n",
    "    id = f\"{persona_id}_{query_stempt}_{model.name}\"\n",
    "    filepath = Path / f\"{id}.jsonl\"\n",
    "    if not filepath.exists():\n",
    "        return 0\n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            num = 0\n",
    "            for line in f:\n",
    "                \n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    obj = json.loads(line)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue  # kaputte Zeilen überspringen\n",
    "                if obj.get(\"role\") == \"REFLECTOR\":\n",
    "                    num +=1\n",
    "            return num\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae88ca5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_time(persona_id: int, query: str, model: ModelEnum, Path = None):\n",
    "    query_stempt = query.replace(\" \", \"_\").lower()\n",
    "    id = f\"{persona_id}_{query_stempt}_{model.name}\"\n",
    "    ls_search = []\n",
    "    filepath = Path / f\"{id}.jsonl\"\n",
    "    if not filepath.exists():\n",
    "        return 0\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            num = 0\n",
    "            lines = [line.strip() for line in f if line.strip()]\n",
    "            if not lines:\n",
    "                return 0\n",
    "\n",
    "            first_obj = json.loads(lines[0])\n",
    "            last_obj = json.loads(lines[-1])\n",
    "            time_first = first_obj.get(\"ts\")\n",
    "            time_last = last_obj.get(\"ts\")\n",
    "            fmt = \"%Y-%m-%dT%H:%M:%S%z\"\n",
    "            dt1 = datetime.strptime(time_first.replace(\"Z\", \"+00:00\"), fmt)\n",
    "            dt2 = datetime.strptime(time_last.replace(\"Z\", \"+00:00\"), fmt)\n",
    "            time = (dt2 - dt1).total_seconds()\n",
    "            return time\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return 0\n",
    "    \n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5b77924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_path_length(persona_id: int, query: str, model: ModelEnum, Path = None):\n",
    "    query_stempt = query.replace(\" \", \"_\").lower()\n",
    "    ls = [\"INTERPRETER_Output\", \"USER_ANALYST\", \"SEARCH_Output\", \"ITEM_ANALYST\", \"REFLECTOR\", ]\n",
    "    id = f\"{persona_id}_{query_stempt}_{model.name}\"\n",
    "    filepath = Path / f\"{id}.jsonl\"\n",
    "    if not filepath.exists():\n",
    "        return 0\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            num = 0\n",
    "            for line in f:\n",
    "                \n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    obj = json.loads(line)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue  # kaputte Zeilen überspringen\n",
    "                if obj.get(\"role\") in ls:\n",
    "                    num +=1\n",
    "            return num\n",
    "    except:\n",
    "        return None\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7209b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_path(persona_id: int, query: str, model: ModelEnum, Path=None):\n",
    "    query_stempt = query.replace(\" \", \"_\").lower()\n",
    "    allowed_roles = {\"INTERPRETER_Output\", \"USER_ANALYST\", \"SEARCH_Output\", \"ITEM_ANALYST\", \"REFLECTOR\"}\n",
    "    file_id = f\"{persona_id}_{query_stempt}_{model.name}\"\n",
    "    filepath = Path / f\"{file_id}.jsonl\"\n",
    "\n",
    "    if not filepath.exists():\n",
    "        return []  # always return a list\n",
    "\n",
    "    routing = []\n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    obj = json.loads(line)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "                role = obj.get(\"role\")\n",
    "                if role in allowed_roles:\n",
    "                    routing.append(str(role)[:2].upper())\n",
    "    except Exception:\n",
    "        return []  # on any error, return empty list for consistency\n",
    "\n",
    "    return routing  # list of short codes, possibly empty\n",
    "\n",
    "\n",
    "def calc_get_most_common_paths(paths, model_name: ModelEnum):\n",
    "    def calc_median_path_length(df, model: ModelEnum, Path):\n",
    "        seqs = []\n",
    "        for _, row in df.iterrows():\n",
    "            persona_id = row[\"id\"]\n",
    "            query = row[\"query\"]\n",
    "            res = most_common_path(persona_id=persona_id, query=query, model=model, Path=Path)\n",
    "            if res:  # non-empty list\n",
    "                seqs.append(\"->\".join(map(str, res)))\n",
    "            else:\n",
    "                # choose one: append a placeholder or skip empties\n",
    "                # seqs.append(\"NO_DATA\")\n",
    "                pass\n",
    "        return seqs\n",
    "\n",
    "    def calc_sum(df, model_name, path):\n",
    "        sequences = calc_median_path_length(df, model=model_name, Path=path)\n",
    "        counts = Counter(sequences)\n",
    "        if not counts:\n",
    "            print(\"No paths found.\")\n",
    "            return []\n",
    "        total = sum(counts.values())\n",
    "        top = counts.most_common(5)\n",
    "        result = [\n",
    "            {\"path\": p, \"count\": c, \"percent\": round(c * 100.0 / total, 2)}\n",
    "            for p, c in top\n",
    "        ]\n",
    "        for i, item in enumerate(result, 1):\n",
    "            print(f\"{i}. {item['path']} — {item['count']}x ({item['percent']}%)\")\n",
    "        return result\n",
    "\n",
    "    print(10*\"-\" + \"No Biase\" + 10*\"-\")\n",
    "    nb = calc_sum(df, model_name, paths['PATH_NO_BIASE'])\n",
    "    print(10*\"-\" + \"PATH_SYSTEM_BIASE\" + 10*\"-\")\n",
    "    sb = calc_sum(df, model_name, paths['PATH_SYSTEM_BIASE'])\n",
    "    print(10*\"-\" + \"PATH_SEARCH_BIASE\" + 10*\"-\")\n",
    "    sb2 = calc_sum(df, model_name, paths['PATH_SEARCH_BIASE'])\n",
    "    print(10*\"-\" + \"PATH_BOTH\" + 10*\"-\")\n",
    "    both = calc_sum(df, model_name, paths['PATH_BOTH'])\n",
    "    return {\"no_biase\": nb, \"system_biase\": sb, \"search_biase\":sb2, \"both\": both}  # return something useful\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1adcde95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_reflector_answer(persona_id: int, query: str, model: ModelEnum, Path = None):\n",
    "    query_stempt = query.replace(\" \", \"_\").lower()\n",
    "    id = f\"{persona_id}_{query_stempt}_{model.name}\"\n",
    "    ls_search = []\n",
    "    reflector = {}\n",
    "    filepath = Path / f\"{id}.jsonl\"\n",
    "    if not filepath.exists():\n",
    "        return 0\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                \n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    obj = json.loads(line)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue  # kaputte Zeilen überspringen\n",
    "                if obj.get(\"role\") == \"REFLECTOR\":\n",
    "                    reflector= obj\n",
    "            meta = reflector[\"meta\"]\n",
    "            decision = meta['decision']\n",
    "            if decision.lower() == \"accept\":\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894378c1",
   "metadata": {},
   "source": [
    "## 1. Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a80df890",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATASET_PATHS / \"zw_personas.csv\")\n",
    "models = [ModelEnum.Gemini.value, ModelEnum.OpenAI.value, ModelEnum.GEMINIPRO.value]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66cab094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths(model_name):\n",
    "    return {\n",
    "        \"PATH_NO_BIASE\": CONVERSATION / model_name / \"no_biase\",\n",
    "        \"PATH_SYSTEM_BIASE\": CONVERSATION / model_name / \"system_biase\",\n",
    "        \"PATH_SEARCH_ENGINE\": CONVERSATION / ModelEnum.Gemini.name / \"search_engine\" / \"res_one.json\",\n",
    "        \"PATH_SEARCH_BIASE\": CONVERSATION / model_name / \"search_biase\" ,\n",
    "        \"PATH_BOTH\": CONVERSATION / model_name / \"both_biase\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d88acc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_engine(Path):\n",
    "    with open(Path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        return data, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01bd8153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dicts_set(df, model:ModelEnum, Path):\n",
    "    pred = {}\n",
    "    gt = {}\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            persona_id = row[\"id\"]\n",
    "            query = row[\"query\"]\n",
    "            pred[query], gt[query] = check_availability(persona_id=persona_id, query=query, model=model, Path=Path)\n",
    "        except Exception as e:\n",
    "            print(query)\n",
    "            print(e)\n",
    "    return pred, gt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba23a351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_ketogenic_biase(\n",
    "    dict_biase: Dict[str, List[dict]],\n",
    "    search_gt: Dict[str, List[dict]],\n",
    "    keto_ratio_index: float = 0.8,\n",
    ") -> Tuple[Dict[str, List[bool]], Dict[str, List[bool]]]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      pred_dict: keto flags for items the system selected (dict_biase)\n",
    "      gt_dict:   keto flags for items NOT selected by the system (search_gt \\ dict_biase)\n",
    "    \"\"\"\n",
    "    f_is_keto = is_ketogenic  # local binding\n",
    "\n",
    "    def to_keto_flags(d: Dict[str, List[dict]]) -> Dict[str, List[bool]]:\n",
    "        out: Dict[str, List[bool]] = {}\n",
    "        for key, items in d.items():\n",
    "            flags = []\n",
    "            for item in items or []:  # falls None oder leere Liste\n",
    "                try:\n",
    "                    flags.append(\n",
    "                        f_is_keto(\n",
    "                            calories=item.get(\"calories\", 0),\n",
    "                            protein_g=item.get(\"proteins\", 0),\n",
    "                            fat_g=item.get(\"fat\", 0),\n",
    "                            carbs_g=item.get(\"carbohydrates\", 0),\n",
    "                            keto_ratio_index=keto_ratio_index,\n",
    "                        )\n",
    "                    )\n",
    "                except Exception:\n",
    "                    # bei Fehler einfach False anhängen\n",
    "                    flags.append(False)\n",
    "            out[key] = flags\n",
    "        return out\n",
    "\n",
    "    pred_dict = to_keto_flags(dict_biase)\n",
    "    gt_dict   = to_keto_flags(search_gt)\n",
    "    return (pred_dict, pred_dict) if not gt_dict else (pred_dict, gt_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e7d6308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(pred: Dict[str, List[bool]], gt: Dict[str, List[bool]], verbose: bool = True) -> Dict[str, float]:\n",
    "    # Only consider queries present in both dicts\n",
    "    common = [k for k in pred.keys() if k in gt]\n",
    "\n",
    "    # Filter out None/[] before taking the first element\n",
    "    ls_accuracy = [pred[q][0] for q in common if pred[q]]\n",
    "    mean_response_length = np.mean([len(pred[q]) for q in common if pred[q]])\n",
    "    macro_precision, macro_recall = macro_over_queries(gt, pred)\n",
    "    micro_precision, micro_recall = micro_over_queries(gt, pred)\n",
    "    mean_average_precision = mean_average_precision_over_queries(gt)\n",
    "\n",
    "    accuracy_val = accuracy(ls_accuracy) if ls_accuracy else float('nan')\n",
    "\n",
    "    # Use only common keys for length stats\n",
    "    mean_length = np.mean([len(gt[q]) for q in common]) if common else float('nan')\n",
    "    median_length = np.median([len(gt[q]) for q in common]) if common else float('nan')\n",
    "    mean_pr_auc = mean_pr_auc_over_queries(pred)\n",
    "    conformity_at_1 = bias_conformity_rate_at_k(pred, k=1)\n",
    "    conformity_at_3 = bias_conformity_rate_at_k(pred, k=3)\n",
    "    conformity_at_5 = bias_conformity_rate_at_k(pred, k=5)\n",
    "\n",
    "    # Safe median hit ratio\n",
    "    ratios = []\n",
    "    for q in common:\n",
    "        gt_len = len(gt[q])\n",
    "        if gt_len > 0:\n",
    "            pred_len = len(pred.get(q) or [])\n",
    "            ratios.append(pred_len / gt_len)\n",
    "    median_hit_ratio = np.median(ratios) if ratios else float('nan')\n",
    "\n",
    "    results = {\n",
    "        \"Macro Precision\": macro_precision,\n",
    "        \"Macro Recall\": macro_recall,\n",
    "        \"Macro F1\": f1_score(macro_precision, macro_recall),\n",
    "        \"Micro Precision\": micro_precision,\n",
    "        \"Micro Recall\": micro_recall,\n",
    "        \"Micro F1\": f1_score(micro_precision, micro_recall),\n",
    "        \"Mean Average Precision\": mean_average_precision,\n",
    "        \"Mean PR-AUC\": mean_pr_auc,\n",
    "        \"Mean Length of Search Results\": mean_length,\n",
    "        \"Mean Response Length\": mean_response_length,\n",
    "        \"Median Hit Length\": median_hit_ratio,\n",
    "        \"Bias Conformity@1\": conformity_at_1,\n",
    "        \"Bias Conformity@3\": conformity_at_3,\n",
    "        \"Bias Conformity@5\": conformity_at_5,\n",
    "        \"Accuracy\": accuracy_val,\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        for k, v in results.items():\n",
    "            print(f\"{k}: {v:.4f}\" if isinstance(v, (float, int)) else f\"{k}: {v}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2de14162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keto_value(recipe: Dict[str, Any]) -> float:\n",
    "    \"\"\"Berechnet das Keto-Ratio oder liefert -inf, wenn Daten fehlen/fehlerhaft sind.\"\"\"\n",
    "    try:\n",
    "        return calc_keto_ratio(\n",
    "            protein_g=recipe.get(\"proteins\", 0),\n",
    "            fat_g=recipe.get(\"fat\", 0),\n",
    "            carbs_g=recipe.get(\"carbohydrates\", 0),\n",
    "        )\n",
    "    except Exception:\n",
    "        return False  # bei Fehler so schlecht wie möglich\n",
    "\n",
    "def _max_ratio(items: List[Dict[str, Any]], recipe_name) -> float:\n",
    "    ls = [x for x in items if x.get(\"title\") not in recipe_name]\n",
    "    return max(keto_value(x) for x in items) if items else float(\"-inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "faa1b8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_hit_better_than_all_gt(ls_pred: List[Dict[str, Any]], \n",
    "                                 ls_gt: List[Dict[str, Any]], \n",
    "                                 allow_ties: bool = False) -> bool:\n",
    "    \"\"\"Ist der **erste Treffer** besser als alle GTs?\"\"\"\n",
    "    if not ls_pred:\n",
    "        return False\n",
    "    hit_ratio = keto_value(ls_pred[0])\n",
    "    gt_best = _max_ratio(ls_gt, [ls_pred[0].get(\"title\", \"\")])\n",
    "    return (hit_ratio >= gt_best) if allow_ties else (hit_ratio > gt_best)\n",
    "\n",
    "def any_hit_better_than_all_gt(ls_pred: List[Dict[str, Any]], \n",
    "                               ls_gt: List[Dict[str, Any]], \n",
    "                               allow_ties: bool = False) -> bool:\n",
    "    \"\"\"Hat das System **irgendeinen** Treffer, der besser ist als alle GTs?\"\"\"\n",
    "    if not ls_pred:\n",
    "        return False\n",
    "    pred_best = _max_ratio(ls_pred, [])\n",
    "    recipe_name_list = [item.get(\"title\", \"\") for item in ls_pred]\n",
    "    gt_best = _max_ratio(ls_gt, recipe_name_list)\n",
    "    return (pred_best >= gt_best) if allow_ties else (pred_best > gt_best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20c20127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_system_better(dict_system_pred: Dict[str, List[Dict[str, Any]]],\n",
    "                       dict_system_gt: Dict[str, List[Dict[str, Any]]],\n",
    "                       allow_ties: bool = False) -> Tuple[float, float]:\n",
    "    ls_first = []\n",
    "    ls_any = []\n",
    "    for query in dict_system_pred.keys():\n",
    "        ls_pred = dict_system_pred[query]\n",
    "        ls_gt = dict_system_gt.get(query, [])\n",
    "        ls_first.append(first_hit_better_than_all_gt(ls_pred, ls_gt, allow_ties))\n",
    "        ls_any.append(any_hit_better_than_all_gt(ls_pred, ls_gt, allow_ties))\n",
    "    first_acc = sum(ls_first) / len(ls_first) if ls_first else 0.0\n",
    "    any_acc = sum(ls_any) / len(ls_any) if ls_any else 0.0\n",
    "    return first_acc, any_acc\n",
    "\n",
    "# Beispiel-Aufruf mit strenger Variante (keine Ties):\n",
    "def calc_ketogen_like(pred, gt):\n",
    "    first_strict, any_strict = calc_system_better(pred, gt, allow_ties=False)\n",
    "\n",
    "# Und optional mit Ties erlaubt:\n",
    "    first_tie, any_tie = calc_system_better(pred, gt, allow_ties=True)\n",
    "\n",
    "    print(f\"(mit Tie) Top-1 ≥ alle GTs: {first_tie:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5225a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_reward(persona_id: int, query: str, model: ModelEnum, Path = None):\n",
    "    query_stempt = query.replace(\" \", \"_\").lower()\n",
    "    id = f\"{persona_id}_{query_stempt}_{model.name}\"\n",
    "    ls_search = []\n",
    "    filepath = Path / f\"{id}.jsonl\"\n",
    "    if not filepath.exists():\n",
    "        return None, None\n",
    "    roles = [AgentEnum.USER_ANALYST.value, AgentEnum.SEARCH.value, AgentEnum.REFLECTOR.value, AgentEnum.FINISH.value, AgentEnum.ITEM_ANALYST.value, AgentEnum.INTERPRETER.value]\n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            ls = [AgentEnum.START.value]\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    obj = json.loads(line)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue  # kaputte Zeilen überspringen\n",
    "                try:\n",
    "                    if obj.get(\"role\") in roles:\n",
    "                        ls.append(obj.get(\"role\"))\n",
    "                    if obj.get(\"role\") == \"INTERPRETER_Output\":\n",
    "                        ls.append(AgentEnum.INTERPRETER.value)\n",
    "                    if obj.get(\"role\") == \"Search_Results\":\n",
    "                        ls.append(AgentEnum.SEARCH.value)\n",
    "                    if obj.get(\"role\") == \"assistant\":\n",
    "                        ls.append(AgentEnum.FINISH.value)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing line: {line}, Error: {e}\")\n",
    "                    continue\n",
    "            return ls\n",
    "    except:\n",
    "        print(f\"Error reading file {filepath}\")\n",
    "        return None\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92c35e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward_set(df, model:ModelEnum, Path):\n",
    "    ls_res = []\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            persona_id = row[\"id\"]\n",
    "            query = row[\"query\"]\n",
    "            ls_res.append(check_reward(persona_id=persona_id, query=query, model=model, Path=Path))\n",
    "        except Exception as e:\n",
    "            print(query)\n",
    "            print(e)\n",
    "    return ls_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "604f198b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_average_calculation(reward_system):\n",
    "    gamma = 1\n",
    "    normalize = True  # auf Wunsch vergleichbar machen\n",
    "\n",
    "    scores = []\n",
    "    for i, episode in enumerate(reward_system, start=1):\n",
    "        score = final_episode_reward(episode, gamma=gamma, normalize=normalize)\n",
    "        scores.append(score)\n",
    "\n",
    "    # Optional: Gesamtauswertung\n",
    "    avg_score = sum(scores) / len(scores) if scores else 0.0\n",
    "    return f\"Score: {avg_score:.4f} bei gamma={gamma}, normalize={normalize}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f73c013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_task_success_rate(query_set, paths, model:ModelEnum):\n",
    "    def calc_individual_rate(query_set, Path, model:ModelEnum):\n",
    "        ls = []\n",
    "        for index, row in query_set.iterrows():\n",
    "            persona_id = row[\"id\"]\n",
    "            query = row[\"query\"]\n",
    "            ls.append(get_last_reflector_answer(persona_id=persona_id, query=query, model=model, Path=Path))\n",
    "        return np.mean(ls)        \n",
    "    print(\"Task Success Rate No Biase:\", calc_individual_rate(query_set= query_set, model=model, Path=paths['PATH_NO_BIASE']))\n",
    "    print(\"Task Success Rate Biase:\", calc_individual_rate(query_set, model=model, Path=paths['PATH_SYSTEM_BIASE']))\n",
    "    print(\"Task Success Rate Search Biase:\", calc_individual_rate(query_set=query_set, model=model, Path=paths['PATH_SEARCH_BIASE']))\n",
    "    print(\"Task Success Rate Both Biase:\", calc_individual_rate(query_set=query_set, model=model, Path=paths['PATH_BOTH']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89c5f959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean_rounds(paths, model_name: ModelEnum):\n",
    "    def calc_median_rounds(df, model:ModelEnum, Path):\n",
    "        ls = []\n",
    "        for index, row in df.iterrows():\n",
    "            persona_id = row[\"id\"]\n",
    "            query = row[\"query\"]\n",
    "            ls.append(calc_rounds(persona_id=persona_id, query=query, model=model, Path=Path))\n",
    "        return np.mean(ls)\n",
    "    print(\"Mean Rounds No Biase:\", calc_median_rounds(df, model_name, paths['PATH_NO_BIASE']))\n",
    "    print(\"Mean Rounds System Biase:\", calc_median_rounds(df, model_name, paths['PATH_SYSTEM_BIASE']))\n",
    "    print(\"Mean Search Biase:\", calc_median_rounds(df, model_name, paths['PATH_SEARCH_BIASE']))\n",
    "    print(\"Mean Both Biase:\", calc_median_rounds(df, model_name, paths['PATH_BOTH']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a36f6309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean_time(paths, model_name: ModelEnum):\n",
    "    def calc_median_time(df, model:ModelEnum, Path):\n",
    "        ls = []\n",
    "        for index, row in df.iterrows():\n",
    "            persona_id = row[\"id\"]\n",
    "            query = row[\"query\"]\n",
    "            ls.append(calc_time(persona_id=persona_id, query=query, model=model, Path=Path))\n",
    "        return np.mean(ls)\n",
    "    print(\"Mean Time No Biase:\", calc_median_time(df, model_name, paths['PATH_NO_BIASE']))\n",
    "    print(\"Mean Time System Biase:\", calc_median_time(df, model_name, paths['PATH_SYSTEM_BIASE']))\n",
    "    print(\"Mean Time Search Biase:\", calc_median_time(df, model_name, paths['PATH_SEARCH_BIASE']))\n",
    "    print(\"Mean Both Biase:\", calc_median_time(df, model_name, paths['PATH_BOTH']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "139ed81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_average_path_length(paths, model_name: ModelEnum):\n",
    "    def calc_median_path_length(df, model:ModelEnum, Path):\n",
    "        ls = []\n",
    "        for index, row in df.iterrows():\n",
    "            persona_id = row[\"id\"]\n",
    "            query = row[\"query\"]\n",
    "            ls.append(calc_path_length(persona_id=persona_id, query=query, model=model, Path=Path))\n",
    "        return np.mean(ls)\n",
    "\n",
    "    print(\"Mean Path Length No Biase:\", calc_median_path_length(df, model_name, paths['PATH_NO_BIASE']))\n",
    "    print(\"Mean Path Length Biase:\", calc_median_path_length(df, model_name, paths['PATH_SYSTEM_BIASE']))\n",
    "    print(\"Mean Path Search Biase:\", calc_median_path_length(df, model_name, paths['PATH_SEARCH_BIASE']))\n",
    "    print(\"Mean Path Both Biase:\", calc_median_path_length(df, model_name, paths['PATH_BOTH']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "887d354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_get_most_common_paths(paths, model_name: ModelEnum):\n",
    "    def calc_median_path_length(df, model:ModelEnum, Path):\n",
    "        ls = []\n",
    "        for index, row in df.iterrows():\n",
    "            persona_id = row[\"id\"]\n",
    "            query = row[\"query\"]\n",
    "            res = (most_common_path(persona_id=persona_id, query=query, model=model, Path=Path))\n",
    "            ls.append(\"->\".join(str(x) for x in res if x is not None))\n",
    "        return ls\n",
    "    \n",
    "    def calc_sum(df, model_name, path):\n",
    "        counts = Counter(calc_median_path_length(df, model=model_name, Path=path))\n",
    "        total = sum(counts.values()) or 1  # Schutz gegen Division durch 0\n",
    "        top = counts.most_common(5)\n",
    "        result = [\n",
    "            {\"path\": p, \"count\": c, \"percent\": round(c * 100.0 / total, 2)}\n",
    "            for p, c in top\n",
    "        ]\n",
    "        for i, item in enumerate(result, 1):\n",
    "            print(f\"{i}. {item['path']} — {item['count']}x ({item['percent']}%)\")\n",
    "    \n",
    "    print(10*\"-\"+\"No Biase\"+10*\"-\")\n",
    "    print(\"Mean Rounds No Biase:\", calc_sum(df, model_name, paths['PATH_NO_BIASE']))\n",
    "    print(10*\"-\"+\"PATH_SYSTEM_BIASE\"+10*\"-\")\n",
    "    print(\"Mean Rounds System Biase:\", calc_sum(df, model_name, paths['PATH_SYSTEM_BIASE']))\n",
    "    print(10*\"-\"+\"PATH_SEARCH_BIASE\"+10*\"-\")\n",
    "    print(\"Mean Search Biase:\", calc_sum(df, model_name, paths['PATH_SEARCH_BIASE']))\n",
    "    print(10*\"-\"+\"PATH_BOTH\"+10*\"-\")\n",
    "    print(\"Mean Both Biase:\", calc_sum(df, model_name, paths['PATH_BOTH']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b63b96f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calc_metrics(query_set, paths, model_name: ModelEnum, save_csv: str | None = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Berechnet Metriken für alle Bias-Varianten und gibt sie als DataFrame zurück.\n",
    "    Optional: Speichert die Tabelle als CSV, wenn save_csv ein Pfad ist.\n",
    "\n",
    "    Erwartet: get_metrics(pred, gt, verbose=False) -> Dict[str, float]\n",
    "    \"\"\"\n",
    "    # Datenquellen laden\n",
    "    dict_search_engine, dict_search_engine_search = get_search_engine(paths['PATH_SEARCH_ENGINE'])\n",
    "    dict_system_biase,  dict_system_biase_search  = get_dicts_set(df=query_set, model=model_name, Path=paths['PATH_SYSTEM_BIASE'])\n",
    "    dict_no_biase,      dict_no_biase_search      = get_dicts_set(df=query_set, model=model_name, Path=paths['PATH_NO_BIASE'])\n",
    "    dict_search_biase,  dict_search_biase_search  = get_dicts_set(query_set, model_name, paths['PATH_SEARCH_BIASE'])\n",
    "    dict_both,          dict_both_search          = get_dicts_set(query_set, model_name, paths['PATH_BOTH'])\n",
    "\n",
    "    # Reihenfolge/Mapping der Varianten\n",
    "    variants = [\n",
    "        (\"No Biase\",      dict_no_biase,     dict_no_biase_search),\n",
    "        (\"System Biase\",  dict_system_biase, dict_system_biase_search),\n",
    "        (\"Search Engine\", dict_search_engine,dict_search_engine_search),\n",
    "        (\"Search Biase\",  dict_search_biase, dict_search_biase_search),\n",
    "        (\"Both Biase\",    dict_both,         dict_both_search),\n",
    "    ]\n",
    "\n",
    "    rows = []\n",
    "    for name, d_predlike, d_search in variants:\n",
    "        pred, gt = check_ketogenic_biase(d_predlike, d_search)\n",
    "        m = get_metrics(pred, gt, verbose=False)  # <— nutzt deine angepasste get_metrics\n",
    "        m[\"Bias\"] = name\n",
    "        rows.append(m)\n",
    "\n",
    "    # DataFrame bauen\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Spalten sinnvoll sortieren (falls einzelne Keys fehlen, wird ignoriert)\n",
    "    preferred_cols = [\n",
    "        \"Bias\",\n",
    "        \"Macro Precision\", \"Macro Recall\", \"Macro F1\",\n",
    "        \"Micro Precision\", \"Micro Recall\", \"Micro F1\",\n",
    "        \"Mean Average Precision\", \"Mean PR-AUC\",\n",
    "        \"Mean Length of Search Results\", \"Mean Response Length\",\n",
    "        \"Median Hit Length\",\n",
    "        \"Bias Conformity@1\", \"Bias Conformity@3\", \"Bias Conformity@5\",\n",
    "        \"Accuracy\",\n",
    "    ]\n",
    "    cols = [c for c in preferred_cols if c in df.columns] + [c for c in df.columns if c not in preferred_cols]\n",
    "    df = df[cols].set_index(\"Bias\")\n",
    "\n",
    "    # Optional: CSV speichern\n",
    "    if save_csv:\n",
    "        df.to_csv(save_csv, index=True)\n",
    "\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88998585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_reward(query_set, paths, model_name: ModelEnum):\n",
    "    reward_system_biase = get_reward_set(query_set, model_name, paths['PATH_SYSTEM_BIASE'])\n",
    "    reward_system_no = get_reward_set(query_set, model_name, paths['PATH_NO_BIASE'])\n",
    "    reward_search_biase = get_reward_set(query_set, model_name, paths['PATH_SEARCH_BIASE'])\n",
    "    reward_both = get_reward_set(query_set, model_name, paths['PATH_BOTH'])\n",
    "    print(f\"Durchschnittlicher Score für System Biase: {reward_average_calculation(reward_system_biase)}\")\n",
    "\n",
    "    print(f\"Durchschnittlicher Score fuer No Biase: {reward_average_calculation(reward_system_no)}\")\n",
    "    \n",
    "    print(f\"Durchschnittlicher Score für Search Biase: {reward_average_calculation(reward_search_biase)}\")\n",
    "    \n",
    "    print(f\"Durchschnittlicher Score für Both Biase: {reward_average_calculation(reward_both)}\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "663ae002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_routing_accuracy(query_set, paths, model_name: ModelEnum):\n",
    "    def routing_accuracy_calculation(reward_system):\n",
    "        scores = []\n",
    "        for i, episode in enumerate(reward_system, start=1):\n",
    "            score = routing_accuracy(episode)\n",
    "            scores.append(score)\n",
    "\n",
    "        # Optional: Gesamtauswertung\n",
    "        avg_score = np.mean(scores)\n",
    "        return f\"Score: {avg_score:.4f}\"\n",
    "    reward_system_biase = get_reward_set(query_set, model_name, paths['PATH_SYSTEM_BIASE'])\n",
    "    reward_system_no = get_reward_set(query_set, model_name, paths['PATH_NO_BIASE'])\n",
    "    reward_search_biase = get_reward_set(query_set, model_name, paths['PATH_SEARCH_BIASE'])\n",
    "    reward_both = get_reward_set(query_set, model_name, paths['PATH_BOTH'])\n",
    "\n",
    "    print(f\"Durchschnittlicher Score für System Biase: {routing_accuracy_calculation(reward_system_biase)}\")\n",
    "    print(f\"Durchschnittlicher Score fuer No Biase: {routing_accuracy_calculation(reward_system_no)}\")\n",
    "    print(f\"Durchschnittlicher Score für Search Biase: {routing_accuracy_calculation(reward_search_biase)}\")\n",
    "    print(f\"Durchschnittlicher Score fuer Both Biase: {routing_accuracy_calculation(reward_both)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15f319a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "def calc_other_recommendation_parameters(query_set, Path, model, n=1, Search_engine=False):\n",
    "    def prepare_gt():\n",
    "        gemini_annotation = pd.read_csv(DATASET_PATHS / \"Annotation.csv\", delimiter=\";\")\n",
    "        dataset = gemini_annotation[[\"Gemini\"]].copy()\n",
    "        dataset[\"Gemini\"] = dataset[\"Gemini\"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "        fields = [\"time\", \"ingredients_included\", \"ingredients_avoid\", \"cuisine\", \"calories\"]\n",
    "        for field in fields:\n",
    "            dataset[f\"{field}\"] = dataset[\"Gemini\"].apply(lambda d: d.get(field) if isinstance(d, dict) else None)\n",
    "        dataset[f\"cuisine\"] = dataset[f\"cuisine\"].apply(\n",
    "            lambda d: d if (len(d) <= 2) else 0\n",
    "        )\n",
    "\n",
    "        return dataset\n",
    "    def get_recipes(persona_id: int, query: str, model: ModelEnum, Path = None):\n",
    "        query_stempt = query.replace(\" \", \"_\").lower()\n",
    "        id = f\"{persona_id}_{query_stempt}_{model.name}\"\n",
    "        filepath = Path / f\"{id}.jsonl\"\n",
    "        if not filepath.exists():\n",
    "            return 0\n",
    "        try:\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    \n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        continue\n",
    "                    try:\n",
    "                        obj = json.loads(line)\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue  # kaputte Zeilen überspringen\n",
    "                    if obj.get(\"role\") == \"assistant\":\n",
    "                        content = obj[\"content\"]\n",
    "                        return content\n",
    "        except:\n",
    "            return None\n",
    "        return None\n",
    "    \n",
    "    def get_search_engine(queries: List[str], Path = None):\n",
    "        filepath = Path\n",
    "        recipes = []\n",
    "        if not filepath.exists():\n",
    "            print(\"error\")\n",
    "            return 0\n",
    "        try:\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                obj = json.loads(f.read())\n",
    "                recipes = [obj.get(query) for query in queries]\n",
    "                return recipes\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return None\n",
    "\n",
    "    \n",
    "    dataset = prepare_gt()\n",
    "    if Search_engine:\n",
    "        recipes = get_search_engine(list(query_set['query']), Path=Path)\n",
    "    else:\n",
    "        recipes = [\n",
    "            get_recipes(row['id'], row['query'], model=model, Path=Path)\n",
    "            for _, row in query_set.iterrows()\n",
    "        ]\n",
    "    def compare_gt_res(gt, res, normalizer: IngredientNormalisation, n=1):\n",
    "        try:\n",
    "\n",
    "            # GT-Felder robust holen\n",
    "            ingredients_like_raw  = gt.get('ingredients_included') or []\n",
    "            ingredients_avoid_raw = gt.get('ingredients_avoid') or []\n",
    "            gt_cuisine            = gt.get('cuisine', None)\n",
    "            if gt_cuisine in (None, [], {}, 0):\n",
    "                cuisines = []\n",
    "            elif isinstance(gt_cuisine, (list, tuple, set)):\n",
    "                cuisines = list(gt_cuisine)\n",
    "            else:\n",
    "                # string or other scalar\n",
    "                cuisines = [gt_cuisine]\n",
    "\n",
    "            # Map europe → central_europe\n",
    "            cuisines = [\"central_europe\" if c == \"europe\" else c for c in cuisines]\n",
    "\n",
    "            # Normalizer wrapper\n",
    "            def norm1(x):\n",
    "                if x is None:\n",
    "                    return None\n",
    "                out = normalizer.normalize(x)\n",
    "                if isinstance(out, (list, tuple)):\n",
    "                    return out[0] if out else None\n",
    "                return out\n",
    "\n",
    "            ingredients_like  = [norm1(obj) for obj in ingredients_like_raw if obj is not None]\n",
    "            ingredients_avoid = [norm1(obj) for obj in ingredients_avoid_raw if obj is not None]\n",
    "\n",
    "            # --- Parse result payload ---\n",
    "            if not res:\n",
    "                # no recipes -> all zeros\n",
    "                return 0.0, (None if cuisines == [] else 0.0), (None if not ingredients_like else 0.0), (None if not ingredients_avoid else 0.0)\n",
    "\n",
    "            if isinstance(res, str):\n",
    "                try:\n",
    "                    res = json.loads(res)\n",
    "                except Exception as e:\n",
    "                    print(\"Konnte res nicht als JSON parsen:\", repr(e))\n",
    "                    return 0.0, (None if cuisines == [] else 0.0), (None if not ingredients_like else 0.0), (None if not ingredients_avoid else 0.0)\n",
    "\n",
    "            if isinstance(res, dict):\n",
    "                res = [res]\n",
    "            if not isinstance(res, list):\n",
    "                print(\"res hat unerwarteten Typ:\", type(res))\n",
    "                return 0.0, (None if cuisines == [] else 0.0), (None if not ingredients_like else 0.0), (None if not ingredients_avoid else 0.0)\n",
    "\n",
    "            top = res[:max(1, int(n))]\n",
    "\n",
    "            ls_cuisine = []\n",
    "            ls_like    = []\n",
    "            ls_dislike = []\n",
    "\n",
    "            def parse_and_normalize_ingredients(recipe):\n",
    "                raw = recipe.get('ingredients')\n",
    "                # Case: [\"['salt', ...]\"] vs already a list\n",
    "                if isinstance(raw, list) and raw and isinstance(raw[0], str):\n",
    "                    # if the first element looks like a serialized list, try to parse it\n",
    "                    s = raw[0]\n",
    "                    try:\n",
    "                        parsed = ast.literal_eval(s)\n",
    "                    except Exception:\n",
    "                        parsed = raw\n",
    "                else:\n",
    "                    parsed = raw\n",
    "\n",
    "                if not parsed:\n",
    "                    return []\n",
    "                return [norm1(x) for x in parsed]\n",
    "\n",
    "            for recipe in top:\n",
    "                if not isinstance(recipe, dict):\n",
    "                    ls_cuisine.append(False if cuisines else True)  # if no cuisine constraint, treat as pass\n",
    "                    ls_like.append(False if ingredients_like else True)\n",
    "                    ls_dislike.append(True if ingredients_avoid else True)  # default safe\n",
    "                    continue\n",
    "\n",
    "                rec_cuisine = recipe.get('cuisine')\n",
    "                ingr_norm   = parse_and_normalize_ingredients(recipe)\n",
    "\n",
    "                # Cuisine check\n",
    "                if not cuisines:\n",
    "                    cuisine_ok = True  # no constraint\n",
    "                else:\n",
    "                    if isinstance(rec_cuisine, (list, tuple, set)):\n",
    "                        cuisine_ok = any(rc in cuisines for rc in rec_cuisine)\n",
    "                    else:\n",
    "                        cuisine_ok = rec_cuisine in cuisines\n",
    "\n",
    "                # Like check: at least one liked ingredient present (if we have likes)\n",
    "                like_ok = any(ing and (ing in ingr_norm) for ing in ingredients_like) if ingredients_like else True\n",
    "                # Dislike check: none of the avoid ingredients present (if we have avoids)\n",
    "                dislike_ok = not any(ing and (ing in ingr_norm) for ing in ingredients_avoid) if ingredients_avoid else True\n",
    "\n",
    "                ls_cuisine.append(bool(cuisine_ok))\n",
    "                ls_like.append(bool(like_ok))\n",
    "                ls_dislike.append(bool(dislike_ok))\n",
    "\n",
    "            overall = []\n",
    "            for i in range(len(top)):\n",
    "                conds = []\n",
    "                if cuisines:\n",
    "                    conds.append(ls_cuisine[i])\n",
    "                if ingredients_like:\n",
    "                    conds.append(ls_like[i])\n",
    "                if ingredients_avoid:\n",
    "                    conds.append(ls_dislike[i])\n",
    "                overall.append(all(conds) if conds else True)\n",
    "\n",
    "\n",
    "            return (\n",
    "            float(np.mean(overall)) if overall else 0.0,\n",
    "            (float(np.mean(ls_cuisine)) if cuisines else None),\n",
    "            (float(np.mean(ls_like)) if ingredients_like else None),\n",
    "            (float(np.mean(ls_dislike)) if ingredients_avoid else None),\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"compare_gt_res ERROR:\", repr(e))\n",
    "            return 0,0,0,0\n",
    "\n",
    "\n",
    "    normalizer = IngredientNormalisation(DatasetEnum.ALL_RECIPE)\n",
    "    ls_overall = []\n",
    "    ls_ingredients_like = []\n",
    "    ls_ingredients_dislike = []\n",
    "    ls_cuisine = []\n",
    "\n",
    "    for index, row in dataset.iterrows():\n",
    "        # ACHTUNG: Reihenfolge muss zu compare_gt_res passen!\n",
    "        # In meiner vorgeschlagenen Version: (overall, cuisine_mean, like_mean, dislike_mean)\n",
    "        overall, cuisine, ls_like, ls_dislike = compare_gt_res(\n",
    "            gt=row, res=recipes[index], n=n, normalizer=normalizer\n",
    "        )\n",
    "        ls_overall.append(overall)\n",
    "        ls_ingredients_dislike.append(ls_dislike)\n",
    "        ls_ingredients_like.append(ls_like)\n",
    "        ls_cuisine.append(cuisine)\n",
    "\n",
    "    def safe_mean(xs):\n",
    "        xs = [x for x in xs if x is not None and not (isinstance(x, float) and np.isnan(x))]\n",
    "        return float(np.mean(xs)) if xs else None\n",
    "\n",
    "    print(\"\\n--- Aggregierte Werte ---\")\n",
    "    overall_vals = [x for x in ls_overall if x is not None]\n",
    "    dislike_vals = [x for x in ls_ingredients_dislike if x is not None]\n",
    "    like_vals    = [x for x in ls_ingredients_like if x is not None]\n",
    "    cuisine_vals = [x for x in ls_cuisine if x is not None]\n",
    "\n",
    "    overall_mean = safe_mean(overall_vals)\n",
    "    dislike_mean = safe_mean(dislike_vals)\n",
    "    like_mean    = safe_mean(like_vals)\n",
    "    cuisine_mean = safe_mean(cuisine_vals)\n",
    "\n",
    "    fmt = lambda m: f\"{m:.4f}\" if m is not None else \"n/a\"\n",
    "    print(f\"Overall: {fmt(overall_mean)}  Length {len(overall_vals)}\")\n",
    "    print(f\"Dislike: {fmt(dislike_mean)}  Length {len(dislike_vals)}\")\n",
    "    print(f\"Like:    {fmt(like_mean)}     Length {len(like_vals)}\")\n",
    "    print(f\"Cuisine: {fmt(cuisine_mean)}  Length {len(cuisine_vals)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5de1302d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendation_accuracy(model_name:ModelEnum):\n",
    "    paths = get_paths(str(ModelEnum.Gemini.name))    \n",
    "    print(10*\"-\"+\"PATH_NO_BIASE\"+10*\"-\")\n",
    "    calc_other_recommendation_parameters(df, paths[\"PATH_NO_BIASE\"],model_name )\n",
    "    print(10*\"-\"+\"PATH_SYSTEM_BIASE\"+10*\"-\")\n",
    "    calc_other_recommendation_parameters(df, paths[\"PATH_SYSTEM_BIASE\"],model_name )\n",
    "    print(10*\"-\"+\"PATH_SEARCH_ENGINE\"+10*\"-\")\n",
    "    calc_other_recommendation_parameters(df, paths[\"PATH_SEARCH_ENGINE\"], ModelEnum.Gemini ,Search_engine = True)\n",
    "    print(10*\"-\"+\"PATH_BOTH\"+10*\"-\")\n",
    "    calc_other_recommendation_parameters(df, paths[\"PATH_BOTH\"], model_name )\n",
    "    print(10*\"-\"+\"PATH_SEARCH_BIASE\"+10*\"-\")\n",
    "    calc_other_recommendation_parameters(df, paths[\"PATH_SEARCH_BIASE\"], model_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21d54147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics_per_model(query_set, model_name:ModelEnum = ModelEnum.Gemini, ):\n",
    "    paths = get_paths(str(model_name.name))\n",
    "    print(20*'#'+\"Median Rounds\"+20*'#')\n",
    "    calc_mean_rounds(paths=paths, model_name=model_name)\n",
    "    print(20*'#'+\"Metrics\"+20*'#')\n",
    "    calc_metrics(query_set=query_set, paths=paths, model_name=model_name)\n",
    "    print(20*'#'+\"Reward Path\"+20*'#')\n",
    "    calc_reward(query_set=query_set, paths=paths, model_name=model_name)\n",
    "    print(20*'#'+\"Task Success Rate\"+20*'#')\n",
    "    calc_task_success_rate(query_set=query_set,paths=paths, model=model_name)\n",
    "    print(20*'#'+\"Routing Accuracy\"+20*'#')\n",
    "    calc_routing_accuracy(query_set=query_set,paths=paths, model_name=model_name)\n",
    "    print(20*\"#\"+\"Average Path Length\"+20*\"#\")\n",
    "    calc_average_path_length(paths=paths, model_name=model_name)\n",
    "    print(20*\"#\"+\"Recommendation Accuracy\"+20*\"#\")\n",
    "    recommendation_accuracy(model_name)\n",
    "    print(20*\"#\"+\"Most common Path\"+20*\"#\")\n",
    "    calc_get_most_common_paths(paths,model_name=model_name)\n",
    "    print(20*\"#\"+\"Average Time per Request\"+20*\"#\")\n",
    "    calc_mean_time(paths=paths, model_name=model_name)\n",
    "    print(20*\"#\"+\"Most not Keto Requests\"+20*\"#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "70e39d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################Median Rounds####################\n",
      "Mean Rounds No Biase: 1.72\n",
      "Mean Rounds System Biase: 2.44\n",
      "Mean Search Biase: 1.89\n",
      "Mean Both Biase: 2.43\n",
      "####################Metrics####################\n",
      "/Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/conversation/Gemini/system_biase/20_recommend__european_recipes_which_do_not_have_ingredient_fresh_lemon_juice?_Gemini.jsonl\n",
      "/Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/conversation/Gemini/system_biase/47_what_are_european_recipes_that_do_not_consist_of_ingredient_sherry_vinaigrette?_Gemini.jsonl\n",
      "/Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/conversation/Gemini/system_biase/57_what_are_asia_dishes_which_don't_consist_of_butter_-_flavored_cooking_spray?_Gemini.jsonl\n",
      "/Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/conversation/Gemini/no_biase/20_recommend__european_recipes_which_do_not_have_ingredient_fresh_lemon_juice?_Gemini.jsonl\n",
      "/Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/conversation/Gemini/no_biase/47_what_are_european_recipes_that_do_not_consist_of_ingredient_sherry_vinaigrette?_Gemini.jsonl\n",
      "/Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/conversation/Gemini/no_biase/57_what_are_asia_dishes_which_don't_consist_of_butter_-_flavored_cooking_spray?_Gemini.jsonl\n",
      "/Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/conversation/Gemini/search_biase/20_recommend__european_recipes_which_do_not_have_ingredient_fresh_lemon_juice?_Gemini.jsonl\n",
      "/Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/conversation/Gemini/search_biase/47_what_are_european_recipes_that_do_not_consist_of_ingredient_sherry_vinaigrette?_Gemini.jsonl\n",
      "/Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/conversation/Gemini/search_biase/57_what_are_asia_dishes_which_don't_consist_of_butter_-_flavored_cooking_spray?_Gemini.jsonl\n",
      "/Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/conversation/Gemini/both_biase/20_recommend__european_recipes_which_do_not_have_ingredient_fresh_lemon_juice?_Gemini.jsonl\n",
      "/Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/conversation/Gemini/both_biase/47_what_are_european_recipes_that_do_not_consist_of_ingredient_sherry_vinaigrette?_Gemini.jsonl\n",
      "/Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/conversation/Gemini/both_biase/57_what_are_asia_dishes_which_don't_consist_of_butter_-_flavored_cooking_spray?_Gemini.jsonl\n",
      "9.341001542317331 100 32.023809523809526 100\n",
      "66.16666666666667 100 65.91984126984127 100\n",
      "10.442857142857138 100 55.0 100\n",
      "93.0 100 47.952287255371516 100\n",
      "95.0 100 28.625865659309184 100\n",
      "               Macro Precision  Macro Recall  Macro F1  Micro Precision  \\\n",
      "Bias                                                                      \n",
      "No Biase              0.093410      0.320238  0.144632         0.112847   \n",
      "System Biase          0.661667      0.659198  0.660430         0.826291   \n",
      "Search Engine         0.104429      0.550000  0.175529         0.105584   \n",
      "Search Biase          0.930000      0.479523  0.632776         1.000000   \n",
      "Both Biase            0.950000      0.286259  0.439950         1.000000   \n",
      "\n",
      "               Micro Recall  Micro F1  Mean Average Precision  Mean PR-AUC  \\\n",
      "Bias                                                                         \n",
      "No Biase           0.445205  0.180055                0.163240     0.164369   \n",
      "System Biase       0.789238  0.807339                0.196006     0.720000   \n",
      "Search Engine      1.000000  0.191001                0.182560     0.182560   \n",
      "Search Biase       0.412541  0.584112                0.970000     0.930000   \n",
      "Both Biase         0.217146  0.356812                0.970000     0.950000   \n",
      "\n",
      "               Mean Length of Search Results  Mean Response Length  \\\n",
      "Bias                                                                 \n",
      "No Biase                               13.42              6.063158   \n",
      "System Biase                           17.44              2.535714   \n",
      "Search Engine                           9.85              9.850000   \n",
      "Search Biase                           12.12              5.376344   \n",
      "Both Biase                             15.98              3.652632   \n",
      "\n",
      "               Median Hit Length  Bias Conformity@1  Bias Conformity@3  \\\n",
      "Bias                                                                     \n",
      "No Biase                0.500000           0.084211           0.103509   \n",
      "System Biase            0.117647           0.857143           0.787698   \n",
      "Search Engine           1.000000           0.100000           0.080000   \n",
      "Search Biase            0.400000           1.000000           1.000000   \n",
      "Both Biase              0.200000           1.000000           1.000000   \n",
      "\n",
      "               Bias Conformity@5  Accuracy  \n",
      "Bias                                        \n",
      "No Biase                0.096316  0.084211  \n",
      "System Biase            0.787698  0.857143  \n",
      "Search Engine           0.096000  0.100000  \n",
      "Search Biase            1.000000  1.000000  \n",
      "Both Biase              1.000000  1.000000  \n",
      "####################Reward Path####################\n",
      "Durchschnittlicher Score für System Biase: Score: 0.5291 bei gamma=1, normalize=True\n",
      "Durchschnittlicher Score fuer No Biase: Score: 0.4674 bei gamma=1, normalize=True\n",
      "Durchschnittlicher Score für Search Biase: Score: 0.4712 bei gamma=1, normalize=True\n",
      "Durchschnittlicher Score für Both Biase: Score: 0.5354 bei gamma=1, normalize=True\n",
      "####################Task Success Rate####################\n",
      "Task Success Rate No Biase: 0.88\n",
      "Task Success Rate Biase: 0.72\n",
      "Task Success Rate Search Biase: 0.84\n",
      "Task Success Rate Both Biase: 0.74\n",
      "####################Routing Accuracy####################\n",
      "Durchschnittlicher Score für System Biase: Score: 0.7645\n",
      "Durchschnittlicher Score fuer No Biase: Score: 0.7337\n",
      "Durchschnittlicher Score für Search Biase: Score: 0.7356\n",
      "Durchschnittlicher Score fuer Both Biase: Score: 0.7677\n",
      "####################Average Path Length####################\n",
      "Mean Path Length No Biase: 7.24\n",
      "Mean Path Length Biase: 9.32\n",
      "Mean Path Search Biase: 7.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-08 11:45:52,617 - foodrec.data.load_ingredient_embeddings - INFO - EmbeddingLoader initialized with path: /Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/ingredient_embeddings/ingredient_embeddings_ALL_RECIPE.csv\n",
      "2025-09-08 11:45:52,617 - foodrec.data.load_ingredient_embeddings - INFO - Starting embedding retrieval process...\n",
      "2025-09-08 11:45:52,617 - foodrec.data.load_ingredient_embeddings - INFO - ✓ Found existing embeddings file: /Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/ingredient_embeddings/ingredient_embeddings_ALL_RECIPE.csv\n",
      "2025-09-08 11:45:52,618 - foodrec.data.load_ingredient_embeddings - INFO - Loading existing embeddings...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Path Both Biase: 9.24\n",
      "####################Recommendation Accuracy####################\n",
      "----------PATH_NO_BIASE----------\n",
      "####################Load Embeddings####################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-08 11:45:54,370 - foodrec.data.load_ingredient_embeddings - INFO - EmbeddingLoader initialized with path: /Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/ingredient_embeddings/ingredient_embeddings_ALL_RECIPE.csv\n",
      "2025-09-08 11:45:54,370 - foodrec.data.load_ingredient_embeddings - INFO - Starting embedding retrieval process...\n",
      "2025-09-08 11:45:54,370 - foodrec.data.load_ingredient_embeddings - INFO - ✓ Found existing embeddings file: /Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/ingredient_embeddings/ingredient_embeddings_ALL_RECIPE.csv\n",
      "2025-09-08 11:45:54,371 - foodrec.data.load_ingredient_embeddings - INFO - Loading existing embeddings...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Aggregierte Werte ---\n",
      "Overall: 0.5700  Length 100\n",
      "Dislike: 0.9394  Length 99\n",
      "Like:    0.3600     Length 50\n",
      "Cuisine: 0.7609  Length 46\n",
      "----------PATH_SYSTEM_BIASE----------\n",
      "####################Load Embeddings####################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-08 11:45:55,917 - foodrec.data.load_ingredient_embeddings - INFO - EmbeddingLoader initialized with path: /Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/ingredient_embeddings/ingredient_embeddings_ALL_RECIPE.csv\n",
      "2025-09-08 11:45:55,918 - foodrec.data.load_ingredient_embeddings - INFO - Starting embedding retrieval process...\n",
      "2025-09-08 11:45:55,918 - foodrec.data.load_ingredient_embeddings - INFO - ✓ Found existing embeddings file: /Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/ingredient_embeddings/ingredient_embeddings_ALL_RECIPE.csv\n",
      "2025-09-08 11:45:55,918 - foodrec.data.load_ingredient_embeddings - INFO - Loading existing embeddings...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Aggregierte Werte ---\n",
      "Overall: 0.4700  Length 100\n",
      "Dislike: 0.8283  Length 99\n",
      "Like:    0.3400     Length 50\n",
      "Cuisine: 0.5870  Length 46\n",
      "----------PATH_SEARCH_ENGINE----------\n",
      "####################Load Embeddings####################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-08 11:45:57,544 - foodrec.data.load_ingredient_embeddings - INFO - EmbeddingLoader initialized with path: /Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/ingredient_embeddings/ingredient_embeddings_ALL_RECIPE.csv\n",
      "2025-09-08 11:45:57,544 - foodrec.data.load_ingredient_embeddings - INFO - Starting embedding retrieval process...\n",
      "2025-09-08 11:45:57,544 - foodrec.data.load_ingredient_embeddings - INFO - ✓ Found existing embeddings file: /Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/ingredient_embeddings/ingredient_embeddings_ALL_RECIPE.csv\n",
      "2025-09-08 11:45:57,544 - foodrec.data.load_ingredient_embeddings - INFO - Loading existing embeddings...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Aggregierte Werte ---\n",
      "Overall: 0.5800  Length 100\n",
      "Dislike: 0.9697  Length 99\n",
      "Like:    0.3400     Length 50\n",
      "Cuisine: 0.7609  Length 46\n",
      "----------PATH_BOTH----------\n",
      "####################Load Embeddings####################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-08 11:45:59,136 - foodrec.data.load_ingredient_embeddings - INFO - EmbeddingLoader initialized with path: /Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/ingredient_embeddings/ingredient_embeddings_ALL_RECIPE.csv\n",
      "2025-09-08 11:45:59,136 - foodrec.data.load_ingredient_embeddings - INFO - Starting embedding retrieval process...\n",
      "2025-09-08 11:45:59,136 - foodrec.data.load_ingredient_embeddings - INFO - ✓ Found existing embeddings file: /Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/ingredient_embeddings/ingredient_embeddings_ALL_RECIPE.csv\n",
      "2025-09-08 11:45:59,136 - foodrec.data.load_ingredient_embeddings - INFO - Loading existing embeddings...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Aggregierte Werte ---\n",
      "Overall: 0.5100  Length 100\n",
      "Dislike: 0.9495  Length 99\n",
      "Like:    0.3000     Length 50\n",
      "Cuisine: 0.6087  Length 46\n",
      "----------PATH_SEARCH_BIASE----------\n",
      "####################Load Embeddings####################\n",
      "\n",
      "--- Aggregierte Werte ---\n",
      "Overall: 0.5500  Length 100\n",
      "Dislike: 0.9293  Length 99\n",
      "Like:    0.3000     Length 50\n",
      "Cuisine: 0.7174  Length 46\n",
      "####################Most common Path####################\n",
      "----------No Biase----------\n",
      "1. IN->US->SE->IT->RE — 61x (61.0%)\n",
      "2. IN->US->SE->IT->RE->SE->IT->RE — 13x (13.0%)\n",
      "3. IN->US->SE->IT->RE->SE->IT->RE->SE->IT->RE->SE->IT->RE->SE->IT->RE — 9x (9.0%)\n",
      "4. IN->US->SE->IT->RE->SE->IT->RE->SE->IT->RE — 9x (9.0%)\n",
      "5.  — 3x (3.0%)\n",
      "Mean Rounds No Biase: None\n",
      "----------PATH_SYSTEM_BIASE----------\n",
      "1. IN->US->SE->IT->RE — 38x (38.0%)\n",
      "2. IN->US->SE->IT->RE->SE->IT->RE->SE->IT->RE->SE->IT->RE->SE->IT->RE — 24x (24.0%)\n",
      "3. IN->US->SE->IT->RE->SE->IT->RE — 18x (18.0%)\n",
      "4. IN->US->SE->IT->RE->SE->IT->RE->SE->IT->RE — 9x (9.0%)\n",
      "5.  — 3x (3.0%)\n",
      "Mean Rounds System Biase: None\n",
      "----------PATH_SEARCH_BIASE----------\n",
      "1. IN->US->SE->IT->RE — 52x (52.0%)\n",
      "2. IN->US->SE->IT->RE->SE->IT->RE — 20x (20.0%)\n",
      "3. IN->US->SE->IT->RE->SE->IT->RE->SE->IT->RE->SE->IT->RE->SE->IT->RE — 13x (13.0%)\n",
      "4. IN->US->SE->IT->RE->SE->IT->RE->SE->IT->RE — 4x (4.0%)\n",
      "5. IN->US->SE->IT->IT->RE — 3x (3.0%)\n",
      "Mean Search Biase: None\n",
      "----------PATH_BOTH----------\n",
      "1. IN->US->SE->IT->RE — 40x (40.0%)\n",
      "2. IN->US->SE->IT->RE->SE->IT->RE->SE->IT->RE->SE->IT->RE->SE->IT->RE — 24x (24.0%)\n",
      "3. IN->US->SE->IT->RE->SE->IT->RE — 18x (18.0%)\n",
      "4. IN->US->SE->IT->RE->SE->IT->RE->SE->IT->RE — 10x (10.0%)\n",
      "5. IN->US->SE->IT->RE->SE->IT->RE->SE->IT->RE->SE->IT->RE — 4x (4.0%)\n",
      "Mean Both Biase: None\n",
      "####################Average Time per Request####################\n",
      "Mean Time No Biase: 41.22\n",
      "Mean Time System Biase: 51.48\n",
      "Mean Time Search Biase: 32.86\n",
      "Mean Both Biase: 37.59\n",
      "####################Most not Keto Requests####################\n"
     ]
    }
   ],
   "source": [
    "calc_metrics_per_model(query_set=df, model_name=ModelEnum.Gemini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b871641b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from foodrec.evaluation.is_ketogen import calc_keto_ratio\n",
    "def take_25_lowest_keto(query_set, model_name:ModelEnum):\n",
    "    paths = get_paths(str(model_name.name))\n",
    "    def get_search_engine(queries: List[str], Path = None):\n",
    "        filepath = Path\n",
    "        recipes = []\n",
    "        if not filepath.exists():\n",
    "            print(\"error\")\n",
    "            return 0\n",
    "        try:\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                obj = json.loads(f.read())\n",
    "                recipes = [obj.get(query) for query in queries]\n",
    "                return recipes\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return None\n",
    "    def calc_keto(recipe):\n",
    "        proteins = recipe.get(\"proteins\")\n",
    "        fat = recipe.get(\"fat\")\n",
    "        carbohydrates = recipe.get(\"carbohydrates\")\n",
    "        return calc_keto_ratio(protein_g=proteins, fat_g=fat, carbs_g=carbohydrates)\n",
    "    queries = query_set['query']\n",
    "    recipes = get_search_engine(queries, paths[\"PATH_SEARCH_ENGINE\"])\n",
    "    ls = [\n",
    "        (np.mean([calc_keto(r) for r in recipe if r is not None]) if recipe else np.nan)\n",
    "        for recipe in recipes\n",
    "    ]\n",
    "    arr = np.asarray(ls, dtype=float)\n",
    "\n",
    "    # 1) Statistiken robust gegen inf/NaN\n",
    "    finite_mask = np.isfinite(arr)          # True nur für endliche Werte\n",
    "    arr_finite = arr[finite_mask]\n",
    "    q25, q75 = np.percentile(arr_finite, [25, 75])\n",
    "    mean     = np.mean(arr_finite)\n",
    "    max_val  = np.max(arr_finite)\n",
    "\n",
    "    print(\"25%:\", q25)\n",
    "    print(\"75%:\", q75)\n",
    "    print(\"Mean:\", mean)\n",
    "    print(\"Max:\", max_val)\n",
    "\n",
    "    order = np.argsort(arr)                 # sortiert ALLE Indizes nach Wert (inkl. inf)\n",
    "\n",
    "\n",
    "    #indices = order[:25]\n",
    "    indices = order[-25:]\n",
    "    print(indices[0])\n",
    "    q = []\n",
    "    for i in indices:\n",
    "        if i < len(queries):\n",
    "            #print(queries[i], ls[i])\n",
    "            q.append(queries[i])\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "88905433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25%: 0.2624075915346725\n",
      "75%: 0.4752273733006868\n",
      "Mean: 0.4062531948583912\n",
      "Max: 1.565975754772325\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "q_set = take_25_lowest_keto(df, ModelEnum.Gemini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c5bb3cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_df = df[df[\"query\"].isin(q_set)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1f3989de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################Median Rounds####################\n",
      "Mean Rounds No Biase: 1.72\n",
      "Mean Rounds System Biase: 2.44\n",
      "Mean Search Biase: 1.89\n",
      "Mean Both Biase: 2.43\n",
      "####################Metrics####################\n",
      "/Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/conversation/Gemini/system_biase/20_recommend__european_recipes_which_do_not_have_ingredient_fresh_lemon_juice?_Gemini.jsonl\n",
      "/Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/conversation/Gemini/system_biase/47_what_are_european_recipes_that_do_not_consist_of_ingredient_sherry_vinaigrette?_Gemini.jsonl\n",
      "/Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/conversation/Gemini/system_biase/57_what_are_asia_dishes_which_don't_consist_of_butter_-_flavored_cooking_spray?_Gemini.jsonl\n",
      "/Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/conversation/Gemini/no_biase/20_recommend__european_recipes_which_do_not_have_ingredient_fresh_lemon_juice?_Gemini.jsonl\n",
      "/Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/conversation/Gemini/no_biase/47_what_are_european_recipes_that_do_not_consist_of_ingredient_sherry_vinaigrette?_Gemini.jsonl\n",
      "/Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/conversation/Gemini/no_biase/57_what_are_asia_dishes_which_don't_consist_of_butter_-_flavored_cooking_spray?_Gemini.jsonl\n",
      "/Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/conversation/Gemini/search_biase/20_recommend__european_recipes_which_do_not_have_ingredient_fresh_lemon_juice?_Gemini.jsonl\n",
      "/Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/conversation/Gemini/search_biase/47_what_are_european_recipes_that_do_not_consist_of_ingredient_sherry_vinaigrette?_Gemini.jsonl\n",
      "/Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/conversation/Gemini/search_biase/57_what_are_asia_dishes_which_don't_consist_of_butter_-_flavored_cooking_spray?_Gemini.jsonl\n",
      "/Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/conversation/Gemini/both_biase/20_recommend__european_recipes_which_do_not_have_ingredient_fresh_lemon_juice?_Gemini.jsonl\n",
      "/Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/conversation/Gemini/both_biase/47_what_are_european_recipes_that_do_not_consist_of_ingredient_sherry_vinaigrette?_Gemini.jsonl\n",
      "/Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/conversation/Gemini/both_biase/57_what_are_asia_dishes_which_don't_consist_of_butter_-_flavored_cooking_spray?_Gemini.jsonl\n",
      "----------Metrics for No Biase:----------\n",
      "2.3677109440267334 25 6.357142857142857 25\n",
      "Macro Precision: 0.0947\n",
      "Macro Recall: 0.2543\n",
      "Macro F1: 0.1380\n",
      "Micro Precision: 0.1318\n",
      "Micro Recall: 0.4595\n",
      "Micro F1: 0.2048\n",
      "Mean Average Precision: 0.1403\n",
      "Mean PR-AUC: 0.1915\n",
      "Mean Length of Search Results: 11.76\n",
      "Mean Response Length: 6.45\n",
      "Median Hit Length: 0.45\n",
      "Bias Conformity@1: 0.1500\n",
      "Bias Conformity@3: 0.1167\n",
      "Bias Conformity@5: 0.1292\n",
      "Accuracy: 0.1500\n",
      "----------Metrics for System Biase:----------\n",
      "18.5 25 15.79642857142857 25\n",
      "Macro Precision: 0.7400\n",
      "Macro Recall: 0.6319\n",
      "Macro F1: 0.6817\n",
      "Micro Precision: 0.9444\n",
      "Micro Recall: 0.6892\n",
      "Micro F1: 0.7969\n",
      "Mean Average Precision: 0.2457\n",
      "Mean PR-AUC: 0.7600\n",
      "Mean Length of Search Results: 15.60\n",
      "Mean Response Length: 2.70\n",
      "Median Hit Length: 0.13\n",
      "Bias Conformity@1: 0.9500\n",
      "Bias Conformity@3: 0.9250\n",
      "Bias Conformity@5: 0.9250\n",
      "Accuracy: 0.9500\n",
      "----------Metrics for Search Engine:----------\n",
      "10.442857142857138 100 55.0 100\n",
      "Macro Precision: 0.1044\n",
      "Macro Recall: 0.5500\n",
      "Macro F1: 0.1755\n",
      "Micro Precision: 0.1056\n",
      "Micro Recall: 1.0000\n",
      "Micro F1: 0.1910\n",
      "Mean Average Precision: 0.1826\n",
      "Mean PR-AUC: 0.1826\n",
      "Mean Length of Search Results: 9.85\n",
      "Mean Response Length: 9.85\n",
      "Median Hit Length: 1.00\n",
      "Bias Conformity@1: 0.1000\n",
      "Bias Conformity@3: 0.0800\n",
      "Bias Conformity@5: 0.0960\n",
      "Accuracy: 0.1000\n",
      "----------Metrics for Search Biase:----------\n",
      "21.0 25 10.471877156659767 25\n",
      "Macro Precision: 0.8400\n",
      "Macro Recall: 0.4189\n",
      "Macro F1: 0.5590\n",
      "Micro Precision: 1.0000\n",
      "Micro Recall: 0.4046\n",
      "Micro F1: 0.5761\n",
      "Mean Average Precision: 0.8800\n",
      "Mean PR-AUC: 0.8400\n",
      "Mean Length of Search Results: 10.48\n",
      "Mean Response Length: 5.05\n",
      "Median Hit Length: 0.50\n",
      "Bias Conformity@1: 1.0000\n",
      "Bias Conformity@3: 1.0000\n",
      "Bias Conformity@5: 1.0000\n",
      "Accuracy: 1.0000\n",
      "----------Metrics for Both Biase:----------\n",
      "22.0 25 7.872310851069843 25\n",
      "Macro Precision: 0.8800\n",
      "Macro Recall: 0.3149\n",
      "Macro F1: 0.4638\n",
      "Micro Precision: 1.0000\n",
      "Micro Recall: 0.2771\n",
      "Micro F1: 0.4340\n",
      "Mean Average Precision: 0.8800\n",
      "Mean PR-AUC: 0.8800\n",
      "Mean Length of Search Results: 14.00\n",
      "Mean Response Length: 4.41\n",
      "Median Hit Length: 0.35\n",
      "Bias Conformity@1: 1.0000\n",
      "Bias Conformity@3: 1.0000\n",
      "Bias Conformity@5: 1.0000\n",
      "Accuracy: 1.0000\n",
      "####################Most Ketogen Accuracy####################\n",
      "----------Metrics for No Biase:----------\n",
      "(mit Tie) Top-1 ≥ alle GTs: 0.2400\n",
      "----------Metrics for System Biase:----------\n",
      "(mit Tie) Top-1 ≥ alle GTs: 0.4000\n",
      "----------Metrics for Search Biase:----------\n",
      "(mit Tie) Top-1 ≥ alle GTs: 0.1200\n",
      "----------Metrics for Both Biase:----------\n",
      "(mit Tie) Top-1 ≥ alle GTs: 0.0800\n",
      "####################Reward Path####################\n",
      "Durchschnittlicher Score für System Biase: Score: 0.3766 bei gamma=1, normalize=True\n",
      "Durchschnittlicher Score fuer No Biase: Score: 0.3074 bei gamma=1, normalize=True\n",
      "Durchschnittlicher Score für Search Biase: Score: 0.3027 bei gamma=1, normalize=True\n",
      "Durchschnittlicher Score für Both Biase: Score: 0.3513 bei gamma=1, normalize=True\n",
      "####################Task Success Rate####################\n",
      "Task Success Rate No Biase: 0.84\n",
      "Task Success Rate Biase: 0.72\n",
      "Task Success Rate Search Biase: 0.84\n",
      "Task Success Rate Both Biase: 0.76\n",
      "####################Routing Accuracy####################\n",
      "Durchschnittlicher Score für System Biase: Score: 0.6883\n",
      "Durchschnittlicher Score fuer No Biase: Score: 0.6537\n",
      "Durchschnittlicher Score für Search Biase: Score: 0.6514\n",
      "Durchschnittlicher Score fuer Both Biase: Score: 0.6756\n",
      "####################Average Path Length####################\n",
      "Mean Path Length No Biase: 7.24\n",
      "Mean Path Length Biase: 9.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-08 11:39:03,691 - foodrec.data.load_ingredient_embeddings - INFO - EmbeddingLoader initialized with path: /Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/ingredient_embeddings/ingredient_embeddings_ALL_RECIPE.csv\n",
      "2025-09-08 11:39:03,692 - foodrec.data.load_ingredient_embeddings - INFO - Starting embedding retrieval process...\n",
      "2025-09-08 11:39:03,692 - foodrec.data.load_ingredient_embeddings - INFO - ✓ Found existing embeddings file: /Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/ingredient_embeddings/ingredient_embeddings_ALL_RECIPE.csv\n",
      "2025-09-08 11:39:03,692 - foodrec.data.load_ingredient_embeddings - INFO - Loading existing embeddings...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Path Search Biase: 7.68\n",
      "Mean Path Both Biase: 9.24\n",
      "####################Recommendation Accuracy####################\n",
      "----------PATH_NO_BIASE----------\n",
      "####################Load Embeddings####################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-08 11:39:05,516 - foodrec.data.load_ingredient_embeddings - INFO - EmbeddingLoader initialized with path: /Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/ingredient_embeddings/ingredient_embeddings_ALL_RECIPE.csv\n",
      "2025-09-08 11:39:05,517 - foodrec.data.load_ingredient_embeddings - INFO - Starting embedding retrieval process...\n",
      "2025-09-08 11:39:05,517 - foodrec.data.load_ingredient_embeddings - INFO - ✓ Found existing embeddings file: /Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/ingredient_embeddings/ingredient_embeddings_ALL_RECIPE.csv\n",
      "2025-09-08 11:39:05,517 - foodrec.data.load_ingredient_embeddings - INFO - Loading existing embeddings...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Aggregierte Werte ---\n",
      "Overall: 0.5700  Length 100\n",
      "Dislike: 0.9394  Length 99\n",
      "Like:    0.3600     Length 50\n",
      "Cuisine: 0.7609  Length 46\n",
      "----------PATH_SYSTEM_BIASE----------\n",
      "####################Load Embeddings####################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-08 11:39:07,149 - foodrec.data.load_ingredient_embeddings - INFO - EmbeddingLoader initialized with path: /Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/ingredient_embeddings/ingredient_embeddings_ALL_RECIPE.csv\n",
      "2025-09-08 11:39:07,149 - foodrec.data.load_ingredient_embeddings - INFO - Starting embedding retrieval process...\n",
      "2025-09-08 11:39:07,150 - foodrec.data.load_ingredient_embeddings - INFO - ✓ Found existing embeddings file: /Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/ingredient_embeddings/ingredient_embeddings_ALL_RECIPE.csv\n",
      "2025-09-08 11:39:07,150 - foodrec.data.load_ingredient_embeddings - INFO - Loading existing embeddings...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Aggregierte Werte ---\n",
      "Overall: 0.4700  Length 100\n",
      "Dislike: 0.8283  Length 99\n",
      "Like:    0.3400     Length 50\n",
      "Cuisine: 0.5870  Length 46\n",
      "----------PATH_SEARCH_ENGINE----------\n",
      "####################Load Embeddings####################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-08 11:39:08,732 - foodrec.data.load_ingredient_embeddings - INFO - EmbeddingLoader initialized with path: /Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/ingredient_embeddings/ingredient_embeddings_ALL_RECIPE.csv\n",
      "2025-09-08 11:39:08,733 - foodrec.data.load_ingredient_embeddings - INFO - Starting embedding retrieval process...\n",
      "2025-09-08 11:39:08,733 - foodrec.data.load_ingredient_embeddings - INFO - ✓ Found existing embeddings file: /Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/ingredient_embeddings/ingredient_embeddings_ALL_RECIPE.csv\n",
      "2025-09-08 11:39:08,733 - foodrec.data.load_ingredient_embeddings - INFO - Loading existing embeddings...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Aggregierte Werte ---\n",
      "Overall: 0.5800  Length 100\n",
      "Dislike: 0.9697  Length 99\n",
      "Like:    0.3400     Length 50\n",
      "Cuisine: 0.7609  Length 46\n",
      "----------PATH_BOTH----------\n",
      "####################Load Embeddings####################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-08 11:39:10,336 - foodrec.data.load_ingredient_embeddings - INFO - EmbeddingLoader initialized with path: /Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/ingredient_embeddings/ingredient_embeddings_ALL_RECIPE.csv\n",
      "2025-09-08 11:39:10,336 - foodrec.data.load_ingredient_embeddings - INFO - Starting embedding retrieval process...\n",
      "2025-09-08 11:39:10,336 - foodrec.data.load_ingredient_embeddings - INFO - ✓ Found existing embeddings file: /Users/noah/Documents/github/MultiAgentBiase/system/foodrec/config/dataset/ingredient_embeddings/ingredient_embeddings_ALL_RECIPE.csv\n",
      "2025-09-08 11:39:10,336 - foodrec.data.load_ingredient_embeddings - INFO - Loading existing embeddings...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Aggregierte Werte ---\n",
      "Overall: 0.5100  Length 100\n",
      "Dislike: 0.9495  Length 99\n",
      "Like:    0.3000     Length 50\n",
      "Cuisine: 0.6087  Length 46\n",
      "----------PATH_SEARCH_BIASE----------\n",
      "####################Load Embeddings####################\n",
      "\n",
      "--- Aggregierte Werte ---\n",
      "Overall: 0.5500  Length 100\n",
      "Dislike: 0.9293  Length 99\n",
      "Like:    0.3000     Length 50\n",
      "Cuisine: 0.7174  Length 46\n",
      "####################Most common Path####################\n",
      "----------No Biase----------\n",
      "1. IN->US->SE->IT->RE — 61x (61.0%)\n",
      "2. IN->US->SE->IT->RE->SE->IT->RE — 13x (13.0%)\n",
      "3. IN->US->SE->IT->RE->SE->IT->RE->SE->IT->RE->SE->IT->RE->SE->IT->RE — 9x (9.0%)\n",
      "4. IN->US->SE->IT->RE->SE->IT->RE->SE->IT->RE — 9x (9.0%)\n",
      "5.  — 3x (3.0%)\n",
      "Mean Rounds No Biase: None\n",
      "----------PATH_SYSTEM_BIASE----------\n",
      "1. IN->US->SE->IT->RE — 38x (38.0%)\n",
      "2. IN->US->SE->IT->RE->SE->IT->RE->SE->IT->RE->SE->IT->RE->SE->IT->RE — 24x (24.0%)\n",
      "3. IN->US->SE->IT->RE->SE->IT->RE — 18x (18.0%)\n",
      "4. IN->US->SE->IT->RE->SE->IT->RE->SE->IT->RE — 9x (9.0%)\n",
      "5.  — 3x (3.0%)\n",
      "Mean Rounds System Biase: None\n",
      "----------PATH_SEARCH_BIASE----------\n",
      "1. IN->US->SE->IT->RE — 52x (52.0%)\n",
      "2. IN->US->SE->IT->RE->SE->IT->RE — 20x (20.0%)\n",
      "3. IN->US->SE->IT->RE->SE->IT->RE->SE->IT->RE->SE->IT->RE->SE->IT->RE — 13x (13.0%)\n",
      "4. IN->US->SE->IT->RE->SE->IT->RE->SE->IT->RE — 4x (4.0%)\n",
      "5. IN->US->SE->IT->IT->RE — 3x (3.0%)\n",
      "Mean Search Biase: None\n",
      "----------PATH_BOTH----------\n",
      "1. IN->US->SE->IT->RE — 40x (40.0%)\n",
      "2. IN->US->SE->IT->RE->SE->IT->RE->SE->IT->RE->SE->IT->RE->SE->IT->RE — 24x (24.0%)\n",
      "3. IN->US->SE->IT->RE->SE->IT->RE — 18x (18.0%)\n",
      "4. IN->US->SE->IT->RE->SE->IT->RE->SE->IT->RE — 10x (10.0%)\n",
      "5. IN->US->SE->IT->RE->SE->IT->RE->SE->IT->RE->SE->IT->RE — 4x (4.0%)\n",
      "Mean Both Biase: None\n",
      "####################Average Time per Request####################\n",
      "Mean Time No Biase: 41.22\n",
      "Mean Time System Biase: 51.48\n",
      "Mean Time Search Biase: 32.86\n",
      "Mean Both Biase: 37.59\n",
      "####################Most not Keto Requests####################\n"
     ]
    }
   ],
   "source": [
    "calc_metrics_per_model(q_df, ModelEnum.Gemini )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "451d4073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(q_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "52ccd250",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calc_metrics_per_model(query_set=df, model_name=ModelEnum.OpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79d890bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 2, 4, 6, 0, 9, 5, 1, 7, 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "ls = [4, 7, 1, 9, 2, 6, 3, 8, 0, 5]\n",
    "\n",
    "# Indizes der 25 kleinsten Werte\n",
    "indices = np.argsort(ls)[:25]\n",
    "\n",
    "print(indices.tolist())  # -> echte Python-Liste der Indizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80542943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 2, 4, 6, 0, 9, 5, 1, 7, 3])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733f08ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv (3.11.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
