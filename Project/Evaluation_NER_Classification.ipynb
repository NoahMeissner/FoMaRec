{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94b81dba",
   "metadata": {},
   "source": [
    "# Named Entity Recognition for Ingredients Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284aad36",
   "metadata": {},
   "source": [
    "Noah MeiÃŸner 18.05.2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f398e1f8",
   "metadata": {},
   "source": [
    "This Jupyter Notebook analyse four different Named Entity Recognition possibilities, to see which fits best for this Use Case.\n",
    "\n",
    "We use:\n",
    "1. Handlabelling as Ground Truth\n",
    "2. ChatGPT 4o mini \n",
    "3. Gemini -- free version\n",
    "4. Spacy (train our own model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b664232",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5197f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noah/Documents/github/MultiAgentBiase/Project/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from train_ner import train_model, test_model\n",
    "from request.request_gemini import request\n",
    "from request.request_gpt import chat_with_openai\n",
    "import pandas as pd\n",
    "from DataLoader_Ingredients import DataLoader\n",
    "from prompts import ingredients_extraction\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import ast\n",
    "from data_structure.DataType import DataType\n",
    "from data_structure.model_name import ModelName\n",
    "from loader.load_ner import ner_loader, ner_safer\n",
    "import re\n",
    "from evaluation.kappa_comparison import evaluate_annotations\n",
    "from request.recipe_api import request_api\n",
    "from request.request_modern_bert import Bert\n",
    "from data_structure.paths import RAW_SET, NER_EVAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77adff86",
   "metadata": {},
   "source": [
    "### Create Dataset\n",
    "- The following function loads the dataset and gives unique ingreidents as test and train split\n",
    "- Train 50000\n",
    "- Test 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9efa9585",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_whole = pd.read_csv(RAW_SET)\n",
    "loader_obj = DataLoader(df_whole)\n",
    "train, test = loader_obj.get_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40aa88d",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914e48d6",
   "metadata": {},
   "source": [
    "We now analyse different approaches, starting with Gemini. We split the test Data in 20 ingredients per prompt, for less requests. For the Gemini and OpenAI approach we are using the same prompt, to evaluate the system model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "197b428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_twenty(data):\n",
    "    return [data[i:i+20] for i in range(0, len(data), 20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ae789b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ingredients_extraction.prompt_ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2cff6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_model_output(str):\n",
    "    match = re.search(r'\\[(.*)\\]', str, re.DOTALL)\n",
    "    if match:\n",
    "        cleaned_json = \"[\" + match.group(1).strip() + \"]\"\n",
    "    try:\n",
    "        parsed_data = ast.literal_eval(cleaned_json)\n",
    "        return parsed_data\n",
    "    except Exception as e:\n",
    "        print(\"Fehler beim Parsen:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d0198f",
   "metadata": {},
   "source": [
    "### Gemini "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994b6302",
   "metadata": {},
   "source": [
    "We are using Gemini 2.0 Flash which gurantees free api access from google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce6b8e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(NER_EVAL/ \"Gemini.json\"):\n",
    "    print(\"Start Annotation process\")\n",
    "    packages = pack_twenty(test)\n",
    "    res = []\n",
    "    for p in packages:\n",
    "        try:\n",
    "            new_prompt = prompt.replace(\"$DATA$\", str(p))\n",
    "            res.extend(clean_model_output(request(new_prompt)))\n",
    "        except Exception as e:\n",
    "            print(\"Model Error:\", e)\n",
    "    ner_safer(ModelName.Gemini,DataType.eval,res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb329f4",
   "metadata": {},
   "source": [
    "### ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e11e30",
   "metadata": {},
   "source": [
    "We are using GPT 4omini as a comparison model, which has a good trade off between accuracy and costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08686827",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(NER_EVAL/\"OpenAI.json\"):\n",
    "    print(\"Start Annotation process\")\n",
    "    packages = pack_twenty(test)\n",
    "    res = []\n",
    "    for p in packages:\n",
    "        try:\n",
    "            new_prompt = prompt.replace(\"$DATA$\", str(p))\n",
    "            res.extend(clean_model_output(chat_with_openai(new_prompt)))\n",
    "        except Exception as e:\n",
    "            print(\"Model Error:\", e)\n",
    "    ner_safer(ModelName.OpenAI,DataType.eval,res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad8fda2",
   "metadata": {},
   "source": [
    "### University API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad153966",
   "metadata": {},
   "source": [
    "The University of Regensburg has a open source API for ingredients classification. We are using that as well to identify how well this performs in comparison\n",
    "https://smarthome.ur.de/naehrwertrechner/\n",
    "\n",
    "**! Hint: Because of a different representation of the annotated Data, this approach will be evaluated in the Detection Part**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ebb3726",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(NER_EVAL/\"UNI.json\"):\n",
    "    print(\"Start Annotation process\")\n",
    "    res = []\n",
    "    for p in test:\n",
    "        try:\n",
    "            output = request_api(str(p))\n",
    "            res.append({p:output})\n",
    "            time.sleep(0.5) \n",
    "        except Exception as e:\n",
    "            print(\"Model Error:\", e)\n",
    "    ner_safer(ModelName.UNI,DataType.eval,res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52494e3e",
   "metadata": {},
   "source": [
    "### Spacy and ModernBertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff6d8d8",
   "metadata": {},
   "source": [
    "We annotated 50000 ingredients using Gemini Flash 2.0 and trained a Spacymodel / finetuned Modern Bert on the classification to see how well this is performing in comparison with Gemini and OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec15f23c",
   "metadata": {},
   "source": [
    "#### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cec410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(NER_EVAL/\"Spacy.json\"):\n",
    "    print(\"Start Annotation process\")\n",
    "    res = []\n",
    "    for obj in test:\n",
    "        try:\n",
    "            result = test_model(str(obj))\n",
    "            df = {obj:{\"entities\":result}}\n",
    "            res.append(df)\n",
    "        except Exception as e:\n",
    "            print(\"Model Error:\", e)\n",
    "    ner_safer(ModelName.Spacy,DataType.eval,res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4136c84c",
   "metadata": {},
   "source": [
    "#### ModernBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef0db6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(NER_EVAL/\"ModernBert.json\"):\n",
    "    print(\"Start Annotation process\")\n",
    "    res = []\n",
    "    bert = Bert()\n",
    "    for obj in test:\n",
    "        try:\n",
    "            result = bert.predict_and_return(str(obj))\n",
    "            res.append(result)\n",
    "        except Exception as e:\n",
    "            print(\"Model Error:\", e)\n",
    "    ner_safer(\"ModernBert\",DataType.eval,res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14e80f9",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119fd718",
   "metadata": {},
   "source": [
    "### How is the Evaluation working?\n",
    "1. **Input:**\n",
    "   - Two annotation lists: `ls_one`, `ls_two`\n",
    "\n",
    "2. **make_one_dict()**\n",
    "   - Converts each list into a dictionary:\n",
    "     `{text: [entities]}`\n",
    "\n",
    "3. **prepare_labels_for_texts()**\n",
    "   - Tokenizes each text\n",
    "   - Matches entities to tokens\n",
    "   - Assigns labels for both annotators\n",
    "   - Truncates label sequences to the same length\n",
    "\n",
    "4. **Check class diversity**\n",
    "   - Ensures both label lists contain at least two unique classes\n",
    "\n",
    "5. **Compute Cohen's Kappa**\n",
    "   - Uses `cohen_kappa_score(labels_one, labels_two)`\n",
    "   - Measures inter-annotator agreement adjusted for chance\n",
    "\n",
    "6. **Output:**\n",
    "   - Final Kappa score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14a4fe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_ann = ner_loader(ModelName.OpenAI,DataType.eval)\n",
    "spacy_ann = ner_loader(ModelName.Spacy,DataType.eval)\n",
    "gemini_ann = ner_loader(ModelName.Gemini,DataType.eval)\n",
    "modern_bert = ner_loader(ModelName.ModernBert, DataType.eval)\n",
    "ground_truth = ner_loader(ModelName.GroundTruth, DataType.eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "860682be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth vs. Gemini\n",
      "\n",
      "ğŸ“ˆ Metriken:\n",
      "â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â••\n",
      "â”‚ Metric        â”‚   Score â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ Accuracy      â”‚  0.8475 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ F1            â”‚  0.8461 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ MCC           â”‚  0.8101 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ Jaccard       â”‚  0.7417 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ Cohen's Kappa â”‚  0.806  â”‚\n",
      "â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•›\n",
      "==============================\n",
      "\n",
      "ğŸ§® Konfusionsmatrix:\n",
      "\n",
      "                O  ingredients  number  type  units\n",
      "O            1208          153       9   176    143\n",
      "ingredients    38         1263       4    48      0\n",
      "number          7            4     971     3      1\n",
      "type           17          183       2   525      8\n",
      "units          19           21       9     5    758\n"
     ]
    }
   ],
   "source": [
    "print(\"Ground Truth vs. Gemini\")\n",
    "evaluate_annotations(ground_truth,gemini_ann)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a263fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAi vs. Ground Truth\n",
      "\n",
      "ğŸ“ˆ Metriken:\n",
      "â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â••\n",
      "â”‚ Metric        â”‚   Score â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ Accuracy      â”‚  0.7137 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ F1            â”‚  0.7018 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ MCC           â”‚  0.6389 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ Jaccard       â”‚  0.569  â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ Cohen's Kappa â”‚  0.6295 â”‚\n",
      "â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•›\n",
      "==============================\n",
      "\n",
      "ğŸ§® Konfusionsmatrix:\n",
      "\n",
      "                O  ingredients  number  type  units\n",
      "O            1139          353      21   136    148\n",
      "ingredients   145         1172       4    31      1\n",
      "number         76            5     897     8      0\n",
      "type           63          503       1   161      7\n",
      "units          78           21       1    25    687\n"
     ]
    }
   ],
   "source": [
    "print(\"OpenAi vs. Ground Truth\")\n",
    "evaluate_annotations(ground_truth,openai_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b7a42b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModernBert vs. Ground Truth\n",
      "\n",
      "ğŸ“ˆ Metriken:\n",
      "â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â••\n",
      "â”‚ Metric        â”‚   Score â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ Accuracy      â”‚  0.8444 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ F1            â”‚  0.8392 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ MCC           â”‚  0.8028 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ Jaccard       â”‚  0.7406 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ Cohen's Kappa â”‚  0.7992 â”‚\n",
      "â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•›\n",
      "==============================\n",
      "\n",
      "ğŸ§® Konfusionsmatrix:\n",
      "\n",
      "                O  ingredients  number  type  units\n",
      "O            1379          173       7   110     12\n",
      "ingredients    86         1254       4     9      0\n",
      "number          4            7     969     6      0\n",
      "type          112          264       3   345     11\n",
      "units          22           25       8     3    754\n"
     ]
    }
   ],
   "source": [
    "print(\"ModernBert vs. Ground Truth\")\n",
    "evaluate_annotations(ground_truth, modern_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "896481be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModernBert vs. Spacy\n",
      "\n",
      "ğŸ“ˆ Metriken:\n",
      "â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â••\n",
      "â”‚ Metric        â”‚   Score â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ Accuracy      â”‚  0.788  â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ F1            â”‚  0.7913 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ MCC           â”‚  0.7284 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ Jaccard       â”‚  0.6622 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ Cohen's Kappa â”‚  0.7252 â”‚\n",
      "â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•›\n",
      "==============================\n",
      "\n",
      "ğŸ§® Konfusionsmatrix:\n",
      "\n",
      "                O  ingredients  number  type  units\n",
      "O            1421          101       1   153      5\n",
      "ingredients   282          987       1    83      0\n",
      "number        113            0     868     5      0\n",
      "type           74          153       0   499      9\n",
      "units         174           11       9     6    612\n"
     ]
    }
   ],
   "source": [
    "print(\"ModernBert vs. Spacy\")\n",
    "evaluate_annotations(ground_truth, spacy_ann)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c36f27",
   "metadata": {},
   "source": [
    "#### Cohens Kappa Interpretation\n",
    "\n",
    "| Values(X)       | Interpretation  | \n",
    "|----------------|--------|\n",
    "| X < 0.0      |  Poor Agreement |\n",
    "| 0.0 < X < 0.2      |  Slight Agreement |\n",
    "| 0.2 < X < 0.4     |  Fair Agreement |\n",
    "| 0.41 < X < 0.6     |  Moderate Agreement |\n",
    "| 0.61 < X < 0.8     |  Substantial Agreement |\n",
    "| 0.81 < X < 1.0     |  Almost Perfect Agreement |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c04886",
   "metadata": {},
   "source": [
    "#### Results\n",
    "| Cohen's Kappa         | Gemini Flash 2.0  | GPT 4omini         | Spacy (*) |Modern Bert (*) |\n",
    "|----------------|--------|----------------|--------|--------|\n",
    "| Ground Truth       | 0.806 |0.6295 |0.7252 | 0.7992|\n",
    "\n",
    "(*) trained on Gemini Data\n",
    "\n",
    "- Results show better results for Gemini as annotator in comparison with Gpt 4omini\n",
    "- Gemini shows almost perfect agreement (Table: Cohens Kappa Interpretation)\n",
    "- Modern Bert shows a Substantial Agreement with the Ground Truth but on the border to almost perfect agreement\n",
    "\n",
    "=> We continue using Modern Bert to find the Entities in the Text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
